############################ Alpine Configuration ###########################

# Required
# Defines the number of worker threads that the event subsystem will consume.
# Events occur asynchronously and are processed by the Event subsystem. This
# value should be large enough to handle most production situations without
# introducing much delay, yet small enough not to pose additional load on an
# already resource-constrained server.
# A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This
# can further be tweaked using the alpine.worker.thread.multiplier property.
# Default value is 0.
alpine.worker.threads=0

# Required
# Defines a multiplier that is used to calculate the number of threads used
# by the event subsystem. This property is only used when alpine.worker.threads
# is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)
# 16 worker threads. Default value is 4.
alpine.worker.thread.multiplier=4

# Required
# Defines the path to the data directory. This directory will hold logs,
# keys, and any database or index files along with application-specific
# files or directories.
alpine.data.directory=~/.dependency-track

# Optional
# Defines the path to the secret key to be used for data encryption and decryption.
# The key will be generated upon first startup if it does not exist.
# Default is "<alpine.data.directory>/keys/secret.key".
# alpine.secret.key.path=/var/run/secrets/secret.key

# Optional
# Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.
# The keys will be generated upon first startup if they do not exist.
# Defaults are "<alpine.data.directory>/keys/private.key" and "<alpine.data.directory>/keys/public.key".
# alpine.private.key.path=/var/run/secrets/private.key
# alpine.public.key.path=/var/run/secrets/public.key

# Optional
# Defines the prefix to be used for API keys. A maximum prefix length of 251
# characters is supported.
# The prefix may also be left empty.
alpine.api.key.prefix=odt_

# Required
# Defines the interval (in seconds) to log general heath information.
# If value equals 0, watchdog logging will be disabled.
alpine.watchdog.logging.interval=0

# Required
# Defines the database mode of operation. Valid choices are:
# 'server', 'embedded', and 'external'.
# In server mode, the database will listen for connections from remote
# hosts. In embedded mode, the system will be more secure and slightly
# faster. External mode should be used when utilizing an external
# database server (i.e. mysql, postgresql, etc).
alpine.database.mode=external

# Optional
# Defines the TCP port to use when the database.mode is set to 'server'.
alpine.database.port=9092

# Required
# Specifies the JDBC URL to use when connecting to the database.
#alpine.database.url=jdbc:postgresql://localhost:5432/dtrack

# Required
# Specifies the JDBC driver class to use.
alpine.database.driver=org.postgresql.Driver

# Optional
# Specifies the path (including filename) to where the JDBC driver is located.
# alpine.database.driver.path=/path/to/dbdriver.jar

# Optional
# Specifies the username to use when authenticating to the database.
#alpine.database.username=dtrack

# Optional
# Specifies the password to use when authenticating to the database.
#alpine.database.password=dtrack

# Optional
# Specifies if database migrations should be performed automatically on startup, based on
# the defined object model of the application. This MUST be disabled as Liquibase is used
# for schema migrations.
alpine.database.migration.enabled=false

# Optional
# Specifies if the database connection pool is enabled.
alpine.database.pool.enabled=true

# Optional
# This property controls the maximum size that the pool is allowed to reach,
# including both idle and in-use connections.
# The property can be set globally for both transactional and non-transactional
# connection pools, or for each pool type separately. When both global and pool-specific
# properties are set, the pool-specific properties take precedence.
alpine.database.pool.max.size=20
# alpine.database.pool.tx.max.size=
# alpine.database.pool.nontx.max.size=

# Optional
# This property controls the minimum number of idle connections in the pool.
# This value should be equal to or less than alpine.database.pool.max.size.
# Warning: If the value is less than alpine.database.pool.max.size,
# alpine.database.pool.idle.timeout will have no effect.
# The property can be set globally for both transactional and non-transactional
# connection pools, or for each pool type separately. When both global and pool-specific
# properties are set, the pool-specific properties take precedence.
alpine.database.pool.min.idle=10
# alpine.database.pool.tx.min.idle=
# alpine.database.pool.nontx.min.idle=

# Optional
# This property controls the maximum amount of time that a connection is
# allowed to sit idle in the pool.
# The property can be set globally for both transactional and non-transactional
# connection pools, or for each pool type separately. When both global and pool-specific
# properties are set, the pool-specific properties take precedence.
alpine.database.pool.idle.timeout=300000
# alpine.database.pool.tx.idle.timeout=
# alpine.database.pool.nontx.idle.timeout=

# Optional
# This property controls the maximum lifetime of a connection in the pool.
# An in-use connection will never be retired, only when it is closed will
# it then be removed.
# The property can be set globally for both transactional and non-transactional
# connection pools, or for each pool type separately. When both global and pool-specific
# properties are set, the pool-specific properties take precedence.
alpine.database.pool.max.lifetime=600000
# alpine.database.pool.tx.max.lifetime=
# alpine.database.pool.nontx.max.lifetime=

# Optional
# Controls the 2nd level cache type used by DataNucleus, the Object Relational Mapper (ORM).
# See https://www.datanucleus.org/products/accessplatform_6_0/jdo/persistence.html#cache_level2
# Values supported by Dependency-Track are "soft" (default), "weak", and "none".
#
# Setting this property to "none" may help in reducing the memory footprint of Dependency-Track,
# but has the potential to slow down database operations.
# Size of the cache may be monitored through the "datanucleus_cache_second_level_entries" metric,
# refer to https://docs.dependencytrack.org/getting-started/monitoring/#metrics for details.
#
# DO NOT CHANGE UNLESS THERE IS A GOOD REASON TO.
alpine.datanucleus.cache.level2.type=none

# Optional
# Defines whether database migrations should be executed on startup.
run.migrations=true

# Optional
# Defines the database JDBC URL to use when executing migrations.
# If not set, the value of alpine.database.url will be used.
# Should generally not be set, unless TLS authentication is used,
# and custom connection variables are required.
# database.migration.url=

# Optional
# Defines the database user for executing migrations.
# If not set, the value of alpine.database.username will be used.
# database.migration.username=

# Optional
# Defines the database password for executing migrations.
# If not set, the value of alpine.database.password will be used.
# database.migration.password=

# Required
# Specifies the number of bcrypt rounds to use when hashing a users password.
# The higher the number the more secure the password, at the expense of
# hardware resources and additional time to generate the hash.
alpine.bcrypt.rounds=14

# Required
# Defines if LDAP will be used for user authentication. If enabled,
# alpine.ldap.* properties should be set accordingly.
alpine.ldap.enabled=false

# Optional
# Specifies the LDAP server URL
# Example (Microsoft Active Directory):
#    alpine.ldap.server.url=ldap://ldap.example.com:3268
#    alpine.ldap.server.url=ldaps://ldap.example.com:3269
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.server.url=ldap://ldap.example.com:389
#    alpine.ldap.server.url=ldaps://ldap.example.com:636
alpine.ldap.server.url=ldap://ldap.example.com:389

# Optional
# Specifies the base DN that all queries should search from
alpine.ldap.basedn=dc=example,dc=com

# Optional
# Specifies the LDAP security authentication level to use. Its value is one of
# the following strings: "none", "simple", "strong". If this property is empty
# or unspecified, the behaviour is determined by the service provider.
alpine.ldap.security.auth=simple

# Optional
# If anonymous access is not permitted, specify a username with limited access
# to the directory, just enough to perform searches. This should be the fully
# qualified DN of the user.
alpine.ldap.bind.username=

# Optional
# If anonymous access is not permitted, specify a password for the username
# used to bind.
alpine.ldap.bind.password=

# Optional
# Specifies if the username entered during login needs to be formatted prior
# to asserting credentials against the directory. For Active Directory, the
# userPrincipal attribute typically ends with the domain, whereas the
# samAccountName attribute and other directory server implementations do not.
# The %s variable will be substitued with the username asserted during login.
# Example (Microsoft Active Directory):
#    alpine.ldap.auth.username.format=%s@example.com
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.auth.username.format=%s
alpine.ldap.auth.username.format=%s@example.com

# Optional
# Specifies the Attribute that identifies a users ID
# Example (Microsoft Active Directory):
#    alpine.ldap.attribute.name=userPrincipalName
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.attribute.name=uid
alpine.ldap.attribute.name=userPrincipalName

# Optional
# Specifies the LDAP attribute used to store a users email address
alpine.ldap.attribute.mail=mail

# Optional
# Specifies the LDAP search filter used to retrieve all groups from the
# directory.
# Example (Microsoft Active Directory):
#    alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group))
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.groups.filter=(&(objectClass=groupOfUniqueNames))
alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group))

# Optional
# Specifies the LDAP search filter to use to query a user and retrieve a list
# of groups the user is a member of. The {USER_DN} variable will be substituted
# with the actual value of the users DN at runtime.
# Example (Microsoft Active Directory):
#    alpine.ldap.user.groups.filter=(&(objectClass=group)(objectCategory=Group)(member={USER_DN}))
# Example (Microsoft Active Directory - with nested group support):
#    alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN})
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.user.groups.filter=(&(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))
alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN})

# Optional
# Specifies the LDAP search filter used to search for groups by their name.
# The {SEARCH_TERM} variable will be substituted at runtime.
# Example (Microsoft Active Directory):
#    alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.groups.search.filter=(&(objectClass=groupOfUniqueNames)(cn=*{SEARCH_TERM}*))
alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))

# Optional
# Specifies the LDAP search filter used to search for users by their name.
# The {SEARCH_TERM} variable will be substituted at runtime.
# Example (Microsoft Active Directory):
#    alpine.ldap.users.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
#    alpine.ldap.users.search.filter=(&(objectClass=inetOrgPerson)(cn=*{SEARCH_TERM}*))
alpine.ldap.users.search.filter=(&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))

# Optional
# Specifies if mapped LDAP accounts are automatically created upon successful
# authentication. When a user logs in with valid credentials but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which ldap users can access the
# system and which users cannot. When this value is set to true, a local ldap
# user will be created and mapped to the ldap account automatically. This
# automatic provisioning only affects authentication, not authorization.
alpine.ldap.user.provisioning=false

# Optional
# This option will ensure that team memberships for LDAP users are dynamic and
# synchronized with membership of LDAP groups. When a team is mapped to an LDAP
# group, all local LDAP users will automatically be assigned to the team if
# they are a member of the group the team is mapped to. If the user is later
# removed from the LDAP group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via an
# external directory.
alpine.ldap.team.synchronization=false

# Optional
# HTTP proxy. If the address is set, then the port must be set too.
# alpine.http.proxy.address=proxy.example.com
# alpine.http.proxy.port=8888
# alpine.http.proxy.username=
# alpine.http.proxy.password=
# alpine.no.proxy=localhost,127.0.0.1

# Optional
# HTTP Outbound Connection Timeout Settings. All values are in seconds.
# alpine.http.timeout.connection=30
# alpine.http.timeout.socket=30
# alpine.http.timeout.pool=60

# Optional
# Cross-Origin Resource Sharing (CORS) headers to include in REST responses.
# If 'alpine.cors.enabled' is true, CORS headers will be sent, if false, no
# CORS headers will be sent.
# See Also: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS
# The following are default values
# alpine.cors.enabled=true
# alpine.cors.allow.origin=*
# alpine.cors.allow.methods=GET POST PUT DELETE OPTIONS
# alpine.cors.allow.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *
# alpine.cors.expose.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count
# alpine.cors.allow.credentials=true
# alpine.cors.max.age=3600

# Optional
# Defines whether Prometheus metrics will be exposed.
# If enabled, metrics will be available via the /metrics endpoint.
alpine.metrics.enabled=false

# Optional
# Defines the username required to access metrics.
# Has no effect when alpine.metrics.auth.password is not set.
alpine.metrics.auth.username=

# Optional
# Defines the password required to access metrics.
# Has no effect when alpine.metrics.auth.username is not set.
alpine.metrics.auth.password=

# Required
# Defines if OpenID Connect will be used for user authentication.
# If enabled, alpine.oidc.* properties should be set accordingly.
alpine.oidc.enabled=false

# Optional
# Defines the client ID to be used for OpenID Connect.
# The client ID should be the same as the one configured for the frontend,
# and will only be used to validate ID tokens.
alpine.oidc.client.id=

# Optional
# Defines the issuer URL to be used for OpenID Connect.
# This issuer MUST support provider configuration via the /.well-known/openid-configuration endpoint.
# See also:
# - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata
# - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig
alpine.oidc.issuer=

# Optional
# Defines the name of the claim that contains the username in the provider's userinfo endpoint.
# Common claims are "name", "username", "preferred_username" or "nickname".
# See also: https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse
alpine.oidc.username.claim=name

# Optional
# Specifies if mapped OpenID Connect accounts are automatically created upon successful
# authentication. When a user logs in with a valid access token but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which OpenID Connect users can access the
# system and which users cannot. When this value is set to true, a local OpenID Connect
# user will be created and mapped to the OpenID Connect account automatically. This
# automatic provisioning only affects authentication, not authorization.
alpine.oidc.user.provisioning=false

# Optional
# This option will ensure that team memberships for OpenID Connect users are dynamic and
# synchronized with membership of OpenID Connect groups or assigned roles. When a team is
# mapped to an OpenID Connect group, all local OpenID Connect users will automatically be
# assigned to the team if they are a member of the group the team is mapped to. If the user
# is later removed from the OpenID Connect group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via the identity provider.
# Note that team synchronization is only performed during user provisioning and after successful
# authentication.
alpine.oidc.team.synchronization=false

# Optional
# Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.
# The claim must be an array of strings. Most public identity providers do not support group or role management.
# When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint
# will most likely need to be configured.
alpine.oidc.teams.claim=groups

# Required
kafka.bootstrap.servers=localhost:9092

# Optional
kafka.auto.offset.reset=earliest

# Optional
kafka.num.stream.threads=3

#Optional
kafka.tls.enabled=false

#Optional
kafka.mtls.enabled=false

#Optional
kafka.security.protocol=

#Optional
kafka.truststore.path=

#Optional
kafka.truststore.password=

#Optional
kafka.keystore.path=

#Optional
kafka.keystore.password=

# Optional
kafka.topic.prefix=

# Required
application.id=dtrack-apiserver

# Optional
# Defines the number of deserialization errors deemed to be acceptable in a given time frame.
# Until the threshold is reached, records failing deserialization will be logged and skipped.
# When the threshold is exceeded, further consumption is stopped.
# The interval must be specified in ISO8601 duration notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).
# The default threshold is 5 errors per 30min.
kafka.streams.deserialization.exception.threshold.count=5
kafka.streams.deserialization.exception.threshold.interval=PT30M

# Optional
# Defines the number of production errors deemed to be acceptable in a given time frame.
# Until the threshold is reached, records failing to be produced will be logged and skipped.
# When the threshold is exceeded, further production is stopped.
# Only certain types of errors will be treated this way; Unexpected errors will cause a
# stop of production immediately.
# The interval must be specified in ISO8601 duration notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).
# The default threshold is 5 errors per 30min.
kafka.streams.production.exception.threshold.count=5
kafka.streams.production.exception.threshold.interval=PT30M

# Optional
# Defines the number of times record processing will be retried in case of unhandled, yet transient errors.
# Until the threshold is reached, records fetched since the last successful offset commit will be attempted to be re-processed.
# When the threshold is exceeded, further processing is stopped.
# Only transient errors will be treated this way; Unexpected or non-transient errors will cause a stop of processing immediately.
# The interval must be specified in ISO8601 duration notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).
# The default threshold is 50 errors per 30min.
kafka.streams.transient.processing.exception.threshold.count=50
kafka.streams.transient.processing.exception.threshold.interval=PT30M

# Optional
# Defines the order in which records are being processed.
# Valid options are:
#  * partition
#  * key
#  * unordered
# alpine.kafka.processor.<name>.processing.order=partition

# Optional
# Defines the maximum size of record batches being processed.
# Batch sizes are further limited by the configured processing order:
#  * partition: Number of partitions assigned to this processor
#  * key:       Number of distinct record keys in current consumer poll
#  * unordered: Potentially unlimited
# Will be ignored when the processor is not a batch processor.
# alpine.kafka.processor.<name>.max.batch.size=10

# Optional
# Defines the maximum concurrency with which records are being processed.
# For batch processors, a smaller number can improve efficiency and throughput.
# A value of -1 indicates that the maximum concurrency should be equal to
# the number of partitions in the topic being consumed from.
# alpine.kafka.processor.<name>.max.concurrency=1

# Optional
# Allows for customization of the processor's retry behavior.
# alpine.kafka.processor.<name>.retry.initial.delay.ms=1000
# alpine.kafka.processor.<name>.retry.multiplier=1
# alpine.kafka.processor.<name>.retry.randomization.factor=0.3
# alpine.kafka.processor.<name>.retry.max.delay.ms=60000

# Optional
# Allows for customization of the underlying Kafka consumer.
# Refer to https://kafka.apache.org/documentation/#consumerconfigs for available options.
# alpine.kafka.processor.<name>.consumer.<consumer.config.name>=

# Required
# Configures the Kafka processor responsible for ingesting mirrored vulnerability
# data from the dtrack.vulnerability topic. The processor only occasionally receives
# records, such that high concurrency is usually not justified.
alpine.kafka.processor.vuln.mirror.max.concurrency=-1
alpine.kafka.processor.vuln.mirror.processing.order=partition
alpine.kafka.processor.vuln.mirror.retry.initial.delay.ms=3000
alpine.kafka.processor.vuln.mirror.retry.multiplier=2
alpine.kafka.processor.vuln.mirror.retry.randomization.factor=0.3
alpine.kafka.processor.vuln.mirror.retry.max.delay.ms=180000
alpine.kafka.processor.vuln.mirror.consumer.group.id=dtrack-apiserver-processor
alpine.kafka.processor.vuln.mirror.consumer.auto.offset.reset=earliest

# Required
# Configures the Kafka processor responsible for ingesting mirrored EPSS
# data from the dtrack.epss topic.
alpine.kafka.processor.epss.mirror.max.concurrency=-1
alpine.kafka.processor.epss.mirror.processing.order=partition
alpine.kafka.processor.epss.mirror.retry.initial.delay.ms=3000
alpine.kafka.processor.epss.mirror.retry.multiplier=2
alpine.kafka.processor.epss.mirror.retry.randomization.factor=0.3
alpine.kafka.processor.epss.mirror.retry.max.delay.ms=180000
alpine.kafka.processor.epss.mirror.consumer.group.id=dtrack-apiserver-processor
alpine.kafka.processor.epss.mirror.consumer.auto.offset.reset=earliest
alpine.kafka.processor.epss.mirror.max.batch.size=100

# Required
# Configures the Kafka processor responsible for ingesting repository metadata
# analysis results from the dtrack.repo-meta-analysis.result topic.
alpine.kafka.processor.repo.meta.analysis.result.max.concurrency=-1
alpine.kafka.processor.repo.meta.analysis.result.processing.order=key
alpine.kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms=1000
alpine.kafka.processor.repo.meta.analysis.result.retry.multiplier=2
alpine.kafka.processor.repo.meta.analysis.result.retry.randomization.factor=0.3
alpine.kafka.processor.repo.meta.analysis.result.retry.max.delay.ms=180000
alpine.kafka.processor.repo.meta.analysis.result.consumer.group.id=dtrack-apiserver-processor
alpine.kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset=earliest

# Scheduling tasks after 3 minutes (3*60*1000) of starting application
task.scheduler.initial.delay=180000

# Cron expressions for tasks have the precision of minutes so polling every minute
task.scheduler.polling.interval=60000

#specifies how long the lock should be kept in case the executing node dies.
#This is just a fallback, under normal circumstances the lock is released as soon the tasks finishes.
#Set lockAtMostFor to a value which is much longer than normal execution time. Default value is 15min
#Lock will be extended dynamically till task execution is finished
task.metrics.portfolio.lockAtMostForInMillis=900000
#specifies minimum amount of time for which the lock should be kept.
# Its main purpose is to prevent execution from multiple nodes in case of really short tasks and clock difference between the nodes.
task.metrics.portfolio.lockAtLeastForInMillis=90000
task.metrics.vulnerability.lockAtMostForInMillis=900000
task.metrics.vulnerability.lockAtLeastForInMillis=90000
task.mirror.epss.lockAtMostForInMillis=900000
task.mirror.epss.lockAtLeastForInMillis=90000
task.componentIdentification.lockAtMostForInMillis=900000
task.componentIdentification.lockAtLeastForInMillis=90000
task.ldapSync.lockAtMostForInMillis=900000
task.ldapSync.lockAtLeastForInMillis=90000
task.workflow.state.cleanup.lockAtMostForInMillis=900000
task.workflow.state.cleanup.lockAtLeastForInMillis=900000
task.portfolio.repoMetaAnalysis.lockAtMostForInMillis=900000
task.portfolio.repoMetaAnalysis.lockAtLeastForInMillis=90000
task.portfolio.vulnAnalysis.lockAtMostForInMillis=900000
task.portfolio.vulnAnalysis.lockAtLeastForInMillis=90000
integrityMetaInitializer.lockAtMostForInMillis=900000
integrityMetaInitializer.lockAtLeastForInMillis=90000

#schedule task for 10th minute of every hour
task.cron.metrics.portfolio=10 * * * *
#schedule task for 40th minute of every hour
task.cron.metrics.vulnerability=40 * * * *
#schedule task every 24 hrs at 02:00 UTC
task.cron.mirror.github=0 2 * * *
#schedule task every 24 hrs at 03:00 UTC
task.cron.mirror.osv=0 3 * * *
#schedule task every 24 hrs at 04:00 UTC
task.cron.mirror.nist=0 4 * * *
#schedule task every 24 hrs at 05:00 UTC
task.cron.mirror.epss=0 5 * * *
#schedule task every 6 hrs at 25th min
task.cron.componentIdentification=25 */6 * * *
#schedule task every 6 hrs at 0th min
task.cron.ldapSync=0 */6 * * *
#schedule task every 24 hrs at 01:00 UTC
task.cron.repoMetaAnalysis=0 1 * * *
#schedule task every 24hrs at 06:00 UTC
task.cron.vulnAnalysis=0 6 * * *
#schedule task at 8:05 UTC on Wednesday every week
task.cron.vulnScanCleanUp=5 8 * * 4

#schedule task every 5 minutes
task.cron.vulnerability.policy.bundle.fetch=*/5 * * * *

#schedule task every 24 hrs at 02:00 UTC
task.cron.fortify.ssc.sync=0 2 * * *
#schedule task every 24 hrs at 02:00 UTC
task.cron.defectdojo.sync=0 2 * * *
#schedule task every 24 hrs at 02:00 UTC
task.cron.kenna.sync=0 2 * * *
#schedule task every 15 minutes
task.cron.workflow.state.cleanup=*/15 * * * *
#schedule task at 0 min past every 12th hr
task.cron.integrityInitializer=0 */12 * * *

# Optional
# Defines the number of write operations to perform during BOM processing before changes are flushed to the database.
# Smaller values may lower memory usage of the API server, whereas higher values will improve performance as fewer
# network round-trips to the database are necessary.
bom.upload.processing.trx.flush.threshold=10000

# Optional
# Defines the duration for how long a workflow step is allowed to remain in PENDING state
# after being started. If this duration is exceeded, workflow steps will transition into the TIMED_OUT state.
# If they remain in TIMED_OUT for the same duration, they will transition to the FAILED state.
# The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).
workflow.step.timeout.duration=PT1H

# Optional
# Defines the duration for how long workflow data is being retained, after all steps transitioned into a non-terminal
# state (CANCELLED, COMPLETED, FAILED, NOT_APPLICABLE).
# The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).
workflow.retention.duration=P3D

# Optional
# Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload
# is completed. The intention being that it is then "safe" to query the API for any identified vulnerabilities.
# This is specifically for cases where polling the /api/v1/bom/token/<TOKEN> endpoint is not feasible.
# THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.
tmp.delay.bom.processed.notification=false

# Optional
# Specifies whether the Integrity Initializer shall be enabled.
integrity.initializer.enabled=false
integrity.check.enabled=false

# Optional
# Defines whether vulnerability policy analysis is enabled.
vulnerability.policy.analysis.enabled=false

# Optional
# Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port
# For nginx, the whole url with bundle name needs to be given
vulnerability.policy.bundle.url=http://localhostt:80/bundles/test.zip

# Optional
# Defines the type of source from which policy bundles are being fetched from.
# Required when vulnerability.policy.bundle.url is set.
# Valid options are:
# - NGINX
# - S3
vulnerability.policy.bundle.source.type=NGINX

#For nginx server, if username and bearer token both are provided, basic auth will be used,
# else the auth header will be added based on the not null values
# Optional
# Defines the password to be used for basic authentication against the service hosting the policy bundle.
vulnerability.policy.bundle.auth.password=

# Optional
# Defines the username to be used for basic authentication against the service hosting the policy bundle.
vulnerability.policy.bundle.auth.username=

# Optional
# Defines the token to be used as bearerAuth against the service hosting the policy bundle.
vulnerability.policy.bundle.bearer.token=

# Optional
# S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional
vulnerability.policy.s3.access.key=
vulnerability.policy.s3.secret.key=
vulnerability.policy.s3.bucket.name=
vulnerability.policy.s3.bundle.name=
vulnerability.policy.s3.region=