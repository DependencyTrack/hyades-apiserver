/*
 * This file is part of Dependency-Track.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * SPDX-License-Identifier: Apache-2.0
 * Copyright (c) OWASP Foundation. All Rights Reserved.
 */
package org.dependencytrack.event.kafka.processor;

import alpine.Config;
import alpine.common.logging.Logger;
import alpine.event.framework.ChainableEvent;
import alpine.event.framework.Event;
import com.google.protobuf.Any;
import com.google.protobuf.util.Timestamps;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.dependencytrack.event.ComponentMetricsUpdateEvent;
import org.dependencytrack.event.ComponentPolicyEvaluationEvent;
import org.dependencytrack.event.ProjectMetricsUpdateEvent;
import org.dependencytrack.event.ProjectPolicyEvaluationEvent;
import org.dependencytrack.event.kafka.KafkaEvent;
import org.dependencytrack.event.kafka.KafkaEventConverter;
import org.dependencytrack.event.kafka.KafkaEventDispatcher;
import org.dependencytrack.event.kafka.processor.api.BatchProcessor;
import org.dependencytrack.event.kafka.processor.exception.ProcessingException;
import org.dependencytrack.model.VulnerabilityScan;
import org.dependencytrack.model.WorkflowState;
import org.dependencytrack.model.WorkflowStatus;
import org.dependencytrack.model.WorkflowStep;
import org.dependencytrack.notification.NotificationConstants;
import org.dependencytrack.persistence.jdbi.NotificationSubjectDao;
import org.dependencytrack.persistence.jdbi.VulnerabilityScanDao;
import org.dependencytrack.persistence.jdbi.WorkflowDao;
import org.dependencytrack.proto.notification.v1.BomConsumedOrProcessedSubject;
import org.dependencytrack.proto.notification.v1.Notification;
import org.dependencytrack.proto.notification.v1.ProjectVulnAnalysisCompleteSubject;
import org.dependencytrack.proto.vulnanalysis.v1.ScanResult;
import org.dependencytrack.proto.vulnanalysis.v1.ScanStatus;
import org.jdbi.v3.core.Handle;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.Set;
import java.util.UUID;
import java.util.stream.Collectors;

import static java.lang.Math.toIntExact;
import static org.dependencytrack.common.ConfigKey.TMP_DELAY_BOM_PROCESSED_NOTIFICATION;
import static org.dependencytrack.persistence.jdbi.JdbiFactory.useJdbiTransaction;
import static org.dependencytrack.proto.notification.v1.Group.GROUP_BOM_PROCESSED;
import static org.dependencytrack.proto.notification.v1.Group.GROUP_PROJECT_VULN_ANALYSIS_COMPLETE;
import static org.dependencytrack.proto.notification.v1.Level.LEVEL_INFORMATIONAL;
import static org.dependencytrack.proto.notification.v1.Scope.SCOPE_PORTFOLIO;

/**
 * A {@link BatchProcessor} that records successfully processed {@link ScanResult}s for their
 * corresponding {@link VulnerabilityScan}, and triggers follow-up processes in case a scan
 * is complete (either {@link VulnerabilityScan.Status#COMPLETED} or {@link VulnerabilityScan.Status#FAILED}).
 */
public class ProcessedVulnerabilityScanResultProcessor implements BatchProcessor<String, ScanResult> {

    static final String PROCESSOR_NAME = "vuln.scan.result.processed";

    private static final Logger LOGGER = Logger.getLogger(ProcessedVulnerabilityScanResultProcessor.class);

    private final KafkaEventDispatcher eventDispatcher = new KafkaEventDispatcher();
    private final boolean shouldDispatchBomProcessedNotification;

    public ProcessedVulnerabilityScanResultProcessor() {
        this(Config.getInstance().getPropertyAsBoolean(TMP_DELAY_BOM_PROCESSED_NOTIFICATION));
    }

    ProcessedVulnerabilityScanResultProcessor(final boolean shouldDispatchBomProcessedNotification) {
        this.shouldDispatchBomProcessedNotification = shouldDispatchBomProcessedNotification;
    }

    @Override
    public void process(final List<ConsumerRecord<String, ScanResult>> records) throws ProcessingException {
        LOGGER.debug("Processing %d records".formatted(records.size()));

        final var completedVulnScans = new ArrayList<VulnerabilityScan>();
        final var notifications = new ArrayList<KafkaEvent<?, ?>>();
        useJdbiTransaction(handle -> {
            completedVulnScans.addAll(processScanResults(handle, records));
            notifications.addAll(createVulnAnalysisCompleteNotifications(handle, completedVulnScans));

            if (shouldDispatchBomProcessedNotification) {
                notifications.addAll(createBomProcessedNotifications(handle, completedVulnScans));
            }
        });

        eventDispatcher.dispatchAll(notifications);
        LOGGER.debug("Dispatched %d notifications".formatted(notifications.size()));

        for (final VulnerabilityScan completedVulnScan : completedVulnScans) {
            if (completedVulnScan.getStatus() != VulnerabilityScan.Status.COMPLETED) {
                continue;
            }

            final ChainableEvent metricsUpdateEvent;
            final ChainableEvent policyEvalEvent;

            switch (completedVulnScan.getTargetType()) {
                case COMPONENT -> {
                    LOGGER.debug("Triggering policy evaluation for component %s".formatted(completedVulnScan.getTargetIdentifier()));
                    metricsUpdateEvent = new ComponentMetricsUpdateEvent(completedVulnScan.getTargetIdentifier());
                    policyEvalEvent = new ComponentPolicyEvaluationEvent(completedVulnScan.getTargetIdentifier());
                }
                case PROJECT -> {
                    LOGGER.debug("Triggering policy evaluation for project %s".formatted(completedVulnScan.getTargetIdentifier()));
                    metricsUpdateEvent = new ProjectMetricsUpdateEvent(completedVulnScan.getTargetIdentifier());
                    policyEvalEvent = new ProjectPolicyEvaluationEvent(completedVulnScan.getTargetIdentifier());
                }
                default -> throw new IllegalStateException("""
                        Unexpected vulnerability scan status %s""".formatted(completedVulnScan.getStatus()));
            }

            final UUID workflowToken = completedVulnScan.getToken();
            metricsUpdateEvent.setChainIdentifier(workflowToken);
            policyEvalEvent.setChainIdentifier(workflowToken);
            policyEvalEvent.onFailure(metricsUpdateEvent);
            policyEvalEvent.onSuccess(metricsUpdateEvent);
            Event.dispatch(policyEvalEvent);
        }
    }

    private static List<VulnerabilityScan> processScanResults(final Handle jdbiHandle, final List<ConsumerRecord<String, ScanResult>> records) {
        final List<VulnerabilityScan> completedVulnScans = recordScanResults(jdbiHandle, records);
        LOGGER.debug("Detected completion of %d vulnerability scans".formatted(completedVulnScans.size()));

        final List<WorkflowState> updatedWorkflowSteps = updateWorkflowStates(jdbiHandle, completedVulnScans);
        LOGGER.debug("Updated %s workflow steps".formatted(updatedWorkflowSteps.size()));

        return completedVulnScans;
    }

    private static List<VulnerabilityScan> recordScanResults(final Handle jdbiHandle, final List<ConsumerRecord<String, ScanResult>> records) {
        final Map<String, Aggregate> aggregatesByToken = aggregateScanResults(records);
        LOGGER.debug("Aggregated %d records down to %d unique scans".formatted(records.size(), aggregatesByToken.size()));

        final int numAggregates = aggregatesByToken.size();
        final var tokens = new ArrayList<UUID>(numAggregates);
        final var resultsTotal = new ArrayList<Integer>(numAggregates);
        final var scannerResultsTotal = new ArrayList<Integer>(numAggregates);
        final var scannerResultsFailed = new ArrayList<Integer>(numAggregates);

        for (final Map.Entry<String, Aggregate> entry : aggregatesByToken.entrySet()) {
            tokens.add(UUID.fromString(entry.getKey()));
            resultsTotal.add(entry.getValue().resultsTotal);
            scannerResultsTotal.add(entry.getValue().scannerResultsTotal);
            scannerResultsFailed.add(entry.getValue().scannerResultsFailed);
        }

        final var vulnScanDao = jdbiHandle.attach(VulnerabilityScanDao.class);
        final List<VulnerabilityScan> updatedVulnScans =
                vulnScanDao.updateAll(tokens, resultsTotal, scannerResultsTotal, scannerResultsFailed);

        return updatedVulnScans.stream()
                // Unfortunately we can't perform this filtering in SQL, as RETURNING
                // does not allow a WHERE clause. Tried using a CTE as workaround:
                //   WITH "CTE" AS (UPDATE ... RETURNING ...) SELECT * FROM "CTE"
                // but that didn't return any results at all.
                // The good news is that the query typically modifies only a handful
                // of scans, so we're wasting not too many resources here.
                .filter(vulnScan -> vulnScan.getStatus() == VulnerabilityScan.Status.COMPLETED
                        || vulnScan.getStatus() == VulnerabilityScan.Status.FAILED)
                .toList();
    }

    private static List<WorkflowState> updateWorkflowStates(final Handle jdbiHandle, final List<VulnerabilityScan> completedVulnScans) {
        final int numScans = completedVulnScans.size();
        final var tokens = new ArrayList<UUID>(numScans);
        final var statuses = new ArrayList<WorkflowStatus>(numScans);
        final var failureReasons = new ArrayList<String>(numScans);

        for (final VulnerabilityScan completedVulnScan : completedVulnScans) {
            tokens.add(completedVulnScan.getToken());
            statuses.add(switch (completedVulnScan.getStatus()) {
                case COMPLETED -> WorkflowStatus.COMPLETED;
                case FAILED -> WorkflowStatus.FAILED;
                default -> throw new IllegalStateException("""
                        Unexpected vulnerability scan status %s""".formatted(completedVulnScan.getStatus()));
            });
            failureReasons.add(completedVulnScan.getFailureReason());
        }

        final var workflowDao = jdbiHandle.attach(WorkflowDao.class);
        final List<WorkflowState> updatedWorkflowStates =
                workflowDao.updateAllStates(WorkflowStep.VULN_ANALYSIS, tokens, statuses, failureReasons);

        final List<UUID> failedStepTokens = updatedWorkflowStates.stream()
                .filter(step -> step.getStatus() == WorkflowStatus.FAILED)
                .map(WorkflowState::getToken)
                .toList();
        if (!failedStepTokens.isEmpty()) {
            LOGGER.debug("Cancelling children of %d failed workflow steps".formatted(failedStepTokens.size()));
            workflowDao.cancelAllChildren(WorkflowStep.VULN_ANALYSIS, failedStepTokens);
        }

        return updatedWorkflowStates;
    }

    private static List<KafkaEvent<?, ?>> createVulnAnalysisCompleteNotifications(final Handle jdbiHandle, final List<VulnerabilityScan> completedVulnScans) {
        final var notificationSubjectDao = jdbiHandle.attach(NotificationSubjectDao.class);

        final var notifications = new ArrayList<KafkaEvent<?, ?>>(completedVulnScans.size());
        for (final VulnerabilityScan completedVulnScan : completedVulnScans) {
            final Optional<ProjectVulnAnalysisCompleteSubject> optionalSubject =
                    notificationSubjectDao.getForProjectVulnAnalysisComplete(completedVulnScan);
            if (optionalSubject.isEmpty()) {
                // Project (no longer) exists.
                continue;
            }

            final var notification = Notification.newBuilder()
                    .setScope(SCOPE_PORTFOLIO)
                    .setGroup(GROUP_PROJECT_VULN_ANALYSIS_COMPLETE)
                    .setLevel(LEVEL_INFORMATIONAL)
                    .setTimestamp(Timestamps.now())
                    .setTitle(NotificationConstants.Title.PROJECT_VULN_ANALYSIS_COMPLETE)
                    .setSubject(Any.pack(optionalSubject.get()))
                    .build();

            notifications.add(KafkaEventConverter.convert(notification));
        }

        return notifications;
    }

    private static List<KafkaEvent<?, ?>> createBomProcessedNotifications(final Handle jdbiHandle, final List<VulnerabilityScan> completedVulnScans) {
        // Collect the workflow tokens for all completed scans, as long as they target a project.
        // Dispatching BOM_PROCESSED notifications does not make sense when individual components,
        // or even the entire portfolio was scanned.
        final Set<UUID> workflowTokens = completedVulnScans.stream()
                .filter(vulnScan -> vulnScan.getTargetType() == VulnerabilityScan.TargetType.PROJECT)
                .map(VulnerabilityScan::getToken)
                .collect(Collectors.toSet());
        if (workflowTokens.isEmpty()) {
            LOGGER.debug("None of the possible %d completed vulnerability scans target a project".formatted(completedVulnScans.size()));
            return Collections.emptyList();
        }

        // Ensure that all eligible workflows have a BOM_PROCESSED step with status COMPLETED.
        // For example, a scan triggered via "Reanalyze" button in the UI won't have such as step,
        // hence it doesn't make sense to dispatch a BOM_PROCESSED notification for it.
        final var workflowDao = jdbiHandle.attach(WorkflowDao.class);
        final Set<UUID> workflowTokensWithBomProcessed =
                workflowDao.getTokensByStepAndStateAndTokenAnyOf(WorkflowStep.BOM_PROCESSING, WorkflowStatus.COMPLETED, workflowTokens);
        if (workflowTokensWithBomProcessed.isEmpty()) {
            LOGGER.debug("None of the possible %d workflows have %s steps with status %s"
                    .formatted(workflowTokens.size(), WorkflowStep.BOM_PROCESSING, WorkflowStatus.COMPLETED));
            return Collections.emptyList();
        }

        final var notificationSubjectDao = jdbiHandle.attach(NotificationSubjectDao.class);
        final List<BomConsumedOrProcessedSubject> notificationSubjects =
                notificationSubjectDao.getForDelayedBomProcessed(workflowTokensWithBomProcessed);

        final var notifications = new ArrayList<KafkaEvent<?, ?>>(workflowTokensWithBomProcessed.size());
        notificationSubjects.stream()
                .map(subject -> Notification.newBuilder()
                        .setScope(SCOPE_PORTFOLIO)
                        .setGroup(GROUP_BOM_PROCESSED)
                        .setLevel(LEVEL_INFORMATIONAL)
                        .setTimestamp(Timestamps.now())
                        .setTitle(NotificationConstants.Title.BOM_PROCESSED)
                        .setContent("A %s BOM was processed".formatted(subject.getBom().getFormat()))
                        .setSubject(Any.pack(subject))
                        .build())
                .map(KafkaEventConverter::convert)
                .forEach(notifications::add);

        return notifications;
    }

    private static class Aggregate {
        private int resultsTotal;
        private int scannerResultsTotal;
        private int scannerResultsFailed;
    }

    private static Map<String, Aggregate> aggregateScanResults(final List<ConsumerRecord<String, ScanResult>> records) {
        final var aggregatesByToken = new HashMap<String, Aggregate>();

        for (final ConsumerRecord<String, ScanResult> record : records) {
            aggregatesByToken.compute(record.key(), (token, existingAggregate) -> {
                final Aggregate aggregate = existingAggregate != null ? existingAggregate : new Aggregate();
                aggregate.resultsTotal++;
                aggregate.scannerResultsTotal += record.value().getScannerResultsCount();
                aggregate.scannerResultsFailed += toIntExact(record.value().getScannerResultsList().stream()
                        .filter(scannerResult -> scannerResult.getStatus() == ScanStatus.SCAN_STATUS_FAILED)
                        .count());
                return aggregate;
            });
        }

        return aggregatesByToken;
    }

}
