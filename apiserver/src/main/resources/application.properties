# Defines the number of worker threads that the event subsystem will consume.
# Events occur asynchronously and are processed by the Event subsystem. This
# value should be large enough to handle most production situations without
# introducing much delay, yet small enough not to pose additional load on an
# already resource-constrained server.
# A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This
# can further be tweaked using the alpine.worker.thread.multiplier property.
#
# @category: Task Execution
# @type:     integer
# @required
alpine.worker.threads=0

# Defines a multiplier that is used to calculate the number of threads used
# by the event subsystem. This property is only used when alpine.worker.threads
# is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)
# 16 worker threads.
#
# @category: Task Execution
# @type:     integer
# @required
alpine.worker.thread.multiplier=4

# Defines the path to the data directory. This directory will hold logs,
# keys, and any database or index files along with application-specific
# files or directories.
#
# @category: General
# @type:     string
# @required
alpine.data.directory=~/.dependency-track

# Defines the path to the secret key to be used for data encryption and decryption.
# The key will be generated upon first startup if it does not exist.
#
# @category: General
# @default:  ${alpine.data.directory}/keys/secret.key
# @type:     string
# alpine.secret.key.path=

# Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.
# The keys will be generated upon first startup if they do not exist.
#
# @category: General
# @default:  ${alpine.data.directory}/keys/private.key
# @example:  /var/run/secrets/private.key
# @type:     string
# alpine.private.key.path=

# Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.
# The keys will be generated upon first startup if they do not exist.
#
# @category: General
# @default:  ${alpine.data.directory}/keys/public.key
# @example:  /var/run/secrets/public.key
# @type:     string
# alpine.public.key.path=

# Defines the prefix to be used for API keys. A maximum prefix length of 251
# characters is supported. The prefix may also be left empty.
#
# @category: General
# @type:     string
alpine.api.key.prefix=odt_

# Defines the number of seconds for which JWTs issued by Dependency-Track will be valid for.
#
# @category: General
# @type:     integer
alpine.auth.jwt.ttl.seconds=604800

# Specifies the JDBC URL to use when connecting to the database.
# For best performance, set the `reWriteBatchedInserts` query parameter to `true`.
#
# @category: Database
# @example:  jdbc:postgresql://localhost:5432/dtrack?reWriteBatchedInserts=true
# @type:     string
# @required
alpine.database.url=

# Specifies the JDBC driver class to use.
#
# @category: Database
# @type:     string
# @hidden
alpine.database.driver=org.postgresql.Driver

# Specifies the username to use when authenticating to the database.
#
# @category: Database
# @type:     string
alpine.database.username=dtrack

# Specifies the password to use when authenticating to the database.
#
# @category: Database
# @type:     string
alpine.database.password=dtrack

# Specifies the file to load the database password from.
# If set, takes precedence over alpine.database.password.
#
# @category: Database
# @example:  /var/run/secrets/database-password
# @type:     string
# alpine.database.password.file=

# Specifies if the database connection pool is enabled.
#
# @category: Database
# @type:     boolean
alpine.database.pool.enabled=true

# Specifies if only the transactional connection pool shall be created,
# and re-used for non-transactional operations. This can make sense
# for applications that handle schema migrations outside the ORM,
# and do not use value generation strategies that require database
# connections. The overhead of maintaining two connection pools can
# be avoided in this case.
#
# @category: Database
# @type:     boolean
# @hidden
alpine.database.pool.tx.only=true

# This property controls the maximum size that the pool is allowed to reach,
# including both idle and in-use connections.
#
# @category: Database
# @type:     integer
alpine.database.pool.max.size=20

# This property controls the minimum number of idle connections in the pool.
# This value should be equal to or less than alpine.database.pool.max.size.
# Warning: If the value is less than alpine.database.pool.max.size,
# alpine.database.pool.idle.timeout will have no effect.
#
# @category: Database
# @type:     integer
alpine.database.pool.min.idle=10

# This property controls the maximum amount of time that a connection is
# allowed to sit idle in the pool.
#
# @category: Database
# @type:     integer
alpine.database.pool.idle.timeout=300000

# This property controls the maximum lifetime of a connection in the pool.
# An in-use connection will never be retired, only when it is closed will
# it then be removed.
#
# @category: Database
# @type:     integer
alpine.database.pool.max.lifetime=600000

# Controls the maximum number of ExecutionContext objects that are pooled by DataNucleus.
# The default defined by DataNucleus is 20. However, since Dependency-Track occasionally
# tweaks ExecutionContexts (e.g. to disable auto-flushing for insert-heavy tasks),
# they are not safe for reuse. We thus disable EC pooling completely.
#
# @category: Database
# @type:     integer
# @hidden
alpine.datanucleus.executioncontext.maxidle=0

# Controls the deletion policy utilized by DataNucleus.
# The default `JDO2` does not consider FK constraints for cascading deletes, only `DataNucleus` does.
# Refer to https://www.datanucleus.org/products/accessplatform/jdo/persistence.html#_deleting_an_object for details.
#
# @category:     Database
# @type:         string
# @valid-values: [JDO2, DataNucleus]
# @hidden
alpine.datanucleus.deletionpolicy=DataNucleus

# Defines whether database migrations should be executed on startup.
# <br/><br/>
# From v5.6.0 onwards, migrations are considered part of the initialization tasks.
# Setting init.tasks.enabled to `false` will disable migrations,
# even if database.run.migrations is enabled.
#
# @category: Database
# @type:     boolean
database.run.migrations=true

# Defines whether the application should exit upon successful execution of database migrations.
# Enabling this option makes the application suitable for running as k8s init container.
# Has no effect unless database.run.migrations is `true`.
# <br/><br/>
# From v5.6.0 onwards, usage of init.and.exit should be preferred.
#
# @category: Database
# @type:     boolean
# database.run.migrations.only=false

# Defines the database JDBC URL to use when executing migrations.
# If not set, the value of alpine.database.url will be used.
# Should generally not be set, unless TLS authentication is used,
# and custom connection variables are required.
#
# @category: Database
# @default:  ${alpine.database.url}
# @type:     string
# database.migration.url=

# Defines the database user for executing migrations.
# If not set, the value of alpine.database.username will be used.
#
# @category: Database
# @default:  ${alpine.database.username}
# @type:     string
# database.migration.username=

# Defines the database password for executing migrations.
# If not set, the value of alpine.database.password will be used.
#
# @category: Database
# @default:  ${alpine.database.password}
# @type:     string
# database.migration.password=

# Specifies the number of bcrypt rounds to use when hashing a user's password.
# The higher the number the more secure the password, at the expense of
# hardware resources and additional time to generate the hash.
#
# @category: General
# @type:     integer
# @required
alpine.bcrypt.rounds=14

# Defines if LDAP will be used for user authentication. If enabled,
# `alpine.ldap.*` properties should be set accordingly.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.enabled=false

# Specifies the LDAP server URL.
# <br/><br/>
# Examples (Microsoft Active Directory):
# <ul>
#   <li><code>ldap://ldap.example.com:3268</code></li>
#   <li><code>ldaps://ldap.example.com:3269</code></li>
# </ul>
# Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul>
#   <li><code>ldap://ldap.example.com:389</code></li>
#   <li><code>ldaps://ldap.example.com:636</code></li>
# </ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.server.url=

# Specifies the base DN that all queries should search from
#
# @category: LDAP
# @example:  dc=example,dc=com
# @type:     string
alpine.ldap.basedn=

# Specifies the LDAP security authentication level to use. Its value is one of
# the following strings: "none", "simple", "strong". If this property is empty
# or unspecified, the behaviour is determined by the service provider.
#
# @category:     LDAP
# @type:         enum
# @valid-values: [none, simple, strong]
alpine.ldap.security.auth=simple

# If anonymous access is not permitted, specify a username with limited access
# to the directory, just enough to perform searches. This should be the fully
# qualified DN of the user.
#
# @category: LDAP
# @type:     string
alpine.ldap.bind.username=

# If anonymous access is not permitted, specify a password for the username
# used to bind.
#
# @category: LDAP
# @type:     string
alpine.ldap.bind.password=

# Specifies if the username entered during login needs to be formatted prior
# to asserting credentials against the directory. For Active Directory, the
# userPrincipal attribute typically ends with the domain, whereas the
# samAccountName attribute and other directory server implementations do not.
# The %s variable will be substituted with the username asserted during login.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>%s@example.com</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>%s</code></li></ul>
#
# @category: LDAP
# @example:  %s@example.com
# @type:     string
alpine.ldap.auth.username.format=

# Specifies the Attribute that identifies a users ID.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>userPrincipalName</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>uid</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.attribute.name=userPrincipalName

# Specifies the LDAP attribute used to store a users email address
#
# @category: LDAP
# @type:     string
alpine.ldap.attribute.mail=mail

# Specifies the LDAP search filter used to retrieve all groups from the directory.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group))

# Specifies the LDAP search filter to use to query a user and retrieve a list
# of groups the user is a member of. The `{USER_DN}` variable will be substituted
# with the actual value of the users DN at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>
# Example (Microsoft Active Directory - with nested group support):
# <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN})

# Specifies the LDAP search filter used to search for groups by their name.
# The `{SEARCH_TERM}` variable will be substituted at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames)(cn=*{SEARCH_TERM}*))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))

# Specifies the LDAP search filter used to search for users by their name.
# The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=inetOrgPerson)(cn=*{SEARCH_TERM}*))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.users.search.filter=(&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))

# Specifies if mapped LDAP accounts are automatically created upon successful
# authentication. When a user logs in with valid credentials but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which ldap users can access the
# system and which users cannot. When this value is set to true, a local ldap
# user will be created and mapped to the ldap account automatically. This
# automatic provisioning only affects authentication, not authorization.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.user.provisioning=false

# This option will ensure that team memberships for LDAP users are dynamic and
# synchronized with membership of LDAP groups. When a team is mapped to an LDAP
# group, all local LDAP users will automatically be assigned to the team if
# they are a member of the group the team is mapped to. If the user is later
# removed from the LDAP group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via an
# external directory.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.team.synchronization=false

# HTTP proxy address. If set, then alpine.http.proxy.port must be set too.
#
# @category: HTTP
# @example:  proxy.example.com
# @type:     string
# alpine.http.proxy.address=

# @category: HTTP
# @example:  8888
# @type:     integer
# alpine.http.proxy.port=

# @category: HTTP
# @type:     string
# alpine.http.proxy.username=

# @category: HTTP
# @type:     string
# alpine.http.proxy.password=

# Specifies the file to load the HTTP proxy password from.
# If set, takes precedence over alpine.http.proxy.password.
#
# @category: HTTP
# @example:  /var/run/secrets/http-proxy-password
# @type:     string
# alpine.http.proxy.password.file=

# @category: HTTP
# @example:  localhost,127.0.0.1
# @type:     string
# alpine.no.proxy=

# Defines the connection timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.connection=30

# Defines the socket / read timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.socket=30

# Defines the request timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.pool=60

# Defines whether [Cross Origin Resource Sharing](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
# (CORS) headers shall be included in REST API responses.
#
# @category: CORS
# @type:     boolean
# alpine.cors.enabled=true

# Controls the content of the `Access-Control-Allow-Origin` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.origin=*

# Controls the content of the `Access-Control-Allow-Methods` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.methods=GET POST PUT DELETE OPTIONS

# Controls the content of the `Access-Control-Allow-Headers` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *

# Controls the content of the `Access-Control-Expose-Headers` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.expose.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count

# Controls the content of the `Access-Control-Allow-Credentials` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     boolean
# alpine.cors.allow.credentials=true

# Controls the content of the `Access-Control-Max-Age` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     integer
# alpine.cors.max.age=3600

# Defines whether Prometheus metrics will be exposed.
# If enabled, metrics will be available via the /metrics endpoint.
#
# @category: Observability
# @type:     boolean
alpine.metrics.enabled=false

# Defines the username required to access metrics.
# Has no effect when alpine.metrics.auth.password is not set.
#
# @category: Observability
# @type:     string
alpine.metrics.auth.username=

# Defines the password required to access metrics.
# Has no effect when alpine.metrics.auth.username is not set.
#
# @category: Observability
# @type:     string
alpine.metrics.auth.password=

# Defines if OpenID Connect will be used for user authentication.
# If enabled, `alpine.oidc.*` properties should be set accordingly.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.enabled=false

# Defines the client ID to be used for OpenID Connect.
# The client ID should be the same as the one configured for the frontend,
# and will only be used to validate ID tokens.
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.client.id=

# Defines the issuer URL to be used for OpenID Connect.
# This issuer MUST support provider configuration via the `/.well-known/openid-configuration` endpoint.
# See also:
# <ul>
#   <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li>
#   <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li>
# </ul>
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.issuer=

# Defines the name of the claim that contains the username in the provider's userinfo endpoint.
# Common claims are `name`, `username`, `preferred_username` or `nickname`.
# See also:
# <ul>
#   <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li>
# </ul>
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.username.claim=name

# Specifies if mapped OpenID Connect accounts are automatically created upon successful
# authentication. When a user logs in with a valid access token but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which OpenID Connect users can access the
# system and which users cannot. When this value is set to true, a local OpenID Connect
# user will be created and mapped to the OpenID Connect account automatically. This
# automatic provisioning only affects authentication, not authorization.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.user.provisioning=false

# This option will ensure that team memberships for OpenID Connect users are dynamic and
# synchronized with membership of OpenID Connect groups or assigned roles. When a team is
# mapped to an OpenID Connect group, all local OpenID Connect users will automatically be
# assigned to the team if they are a member of the group the team is mapped to. If the user
# is later removed from the OpenID Connect group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via the identity provider.
# Note that team synchronization is only performed during user provisioning and after successful
# authentication.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.team.synchronization=false

# Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.
# The claim must be an array of strings. Most public identity providers do not support group or role management.
# When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint
# will most likely need to be configured.
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.teams.claim=groups

# @category: Kafka
# @example:  localhost:9092
# @type:     string
# @required
kafka.bootstrap.servers=

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
kafka.auto.offset.reset=earliest

# @category: Kafka
# @type:     boolean
kafka.tls.enabled=false

# @category: Kafka
# @type:     boolean
kafka.mtls.enabled=false

# @category:     Kafka
# @type:         enum
# @valid-values: [PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]
kafka.security.protocol=

# @category: Kafka
# @type:     string
kafka.truststore.path=

# @category: Kafka
# @type:     string
kafka.truststore.password=

# @category: Kafka
# @type:     string
kafka.keystore.path=

# @category: Kafka
# @type:     string
kafka.keystore.password=

# @category: Kafka
# @type:     string
dt.kafka.topic.prefix=

# Defines the order in which records are being processed.
# Valid options are:
#  * partition
#  * key
#  * unordered
# kafka.processor.<name>.processing.order=partition

# Defines the maximum size of record batches being processed.
# Batch sizes are further limited by the configured processing order:
#  * partition: Number of partitions assigned to this processor
#  * key:       Number of distinct record keys in current consumer poll
#  * unordered: Potentially unlimited
# Will be ignored when the processor is not a batch processor.
# kafka.processor.<name>.max.batch.size=10

# Defines the maximum concurrency with which records are being processed.
# For batch processors, a smaller number can improve efficiency and throughput.
# A value of -1 indicates that the maximum concurrency should be equal to
# the number of partitions in the topic being consumed from.
# kafka.processor.<name>.max.concurrency=1

# Allows for customization of the processor's retry behavior.
# kafka.processor.<name>.retry.initial.delay.ms=1000
# kafka.processor.<name>.retry.multiplier=1
# kafka.processor.<name>.retry.randomization.factor=0.3
# kafka.processor.<name>.retry.max.delay.ms=60000

# Defines the timeout to wait for the processor to finish any pending work
# prior to being shut down.
# kafka.processor.<name>.shutdown.timeout.ms=10000

# Allows for customization of the underlying Kafka consumer.
# Refer to https://kafka.apache.org/documentation/#consumerconfigs for available options.
# kafka.processor.<name>.consumer.<consumer.config.name>=

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.mirror.processing.order=partition

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.initial.delay.ms=3000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.mirror.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.mirror.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.mirror.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.epss.mirror.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.epss.mirror.processing.order=key

# @category: Kafka
# @type:     integer
# @required
kafka.processor.epss.mirror.retry.initial.delay.ms=3000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.epss.mirror.retry.multiplier=2

# @category: Kafka
# @type:     double
# @required
kafka.processor.epss.mirror.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.epss.mirror.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.epss.mirror.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.epss.mirror.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.epss.mirror.max.batch.size=500

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.repo.meta.analysis.result.processing.order=key

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.repo.meta.analysis.result.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.repo.meta.analysis.result.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.scan.result.processing.order=key

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.initial.delay.ms=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.scan.result.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.scan.result.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.scan.result.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.max.batch.size=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.max.concurrency=1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.scan.result.processed.processing.order=unordered

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms=3000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.scan.result.processed.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.scan.result.processed.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.consumer.max.poll.records=10000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes=524288

# Scheduling tasks after 3 minutes (3*60*1000) of starting application
#
# @category: Task Scheduling
# @type:     integer
# @required
task.scheduler.initial.delay=180000

# Cron expressions for tasks have the precision of minutes so polling every minute
#
# @category: Task Scheduling
# @type:     integer
# @required
task.scheduler.polling.interval=60000

# Maximum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.portfolio.metrics.update.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.portfolio.metrics.update.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.metrics.update.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.metrics.update.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the internal component identification task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.internal.component.identification.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the internal component identification task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.internal.component.identification.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.ldap.sync.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.ldap.sync.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.repository.meta.analysis.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.repository.meta.analysis.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.analysis.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.analysis.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.integrity.meta.initializer.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.integrity.meta.initializer.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.policy.fetch.lock.max.duration=PT5M

# Minimum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.policy.fetch.lock.min.duration=PT5S

# Cron expression of the portfolio metrics update task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.portfolio.metrics.update.cron=10 * * * *

# Cron expression of the vulnerability metrics update task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.metrics.update.cron=40 * * * *

# Cron expression of the vulnerability GitHub Advisories mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.git.hub.advisory.mirror.cron=0 2 * * *

# Cron expression of the OSV mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.osv.mirror.cron=0 3 * * *

# Cron expression of the NIST / NVD mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.nist.mirror.cron=0 4 * * *

# Cron expression of the EPSS mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.epss.mirror.cron=0 1 * * *

# Cron expression of the internal component identification task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.internal.component.identification.cron=25 */6 * * *

# Cron expression of the LDAP synchronization task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.ldap.sync.cron=0 */6 * * *

# Cron expression of the portfolio repository metadata analysis task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.repository.meta.analysis.cron=0 1 * * *

# Cron expression of the portfolio vulnerability analysis task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.analysis.cron=0 6 * * *

# Cron expression of the vulnerability policy bundle fetch task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.policy.fetch.cron=*/5 * * * *

# Cron expression of the Fortify SSC upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.fortify.ssc.upload.cron=0 2 * * *

# Cron expression of the DefectDojo upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.defect.dojo.upload.cron=0 2 * * *

# Cron expression of the Kenna Security upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.kenna.security.upload.cron=0 2 * * *

# Cron expression of the integrity metadata initializer task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.integrity.meta.initializer.cron=0 */12 * * *

# Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload
# is completed. The intention being that it is then "safe" to query the API for any identified vulnerabilities.
# This is specifically for cases where polling the /api/v1/bom/token/<TOKEN> endpoint is not feasible.
# THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.
#
# @category: General
# @type:     boolean
tmp.delay.bom.processed.notification=false

# Specifies whether the Integrity Initializer shall be enabled.
#
# @category: General
# @type:     boolean
integrity.initializer.enabled=false

# @category: General
# @type:     boolean
integrity.check.enabled=false

# Defines whether vulnerability policy analysis is enabled.
#
# @category: General
# @type:     boolean
vulnerability.policy.analysis.enabled=false

# Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port
# For nginx, the whole url with bundle name needs to be given
#
# @category: General
# @example:  http://example.com:80/bundles/bundle.zip
# @type:     string
vulnerability.policy.bundle.url=

# Defines the type of source from which policy bundles are being fetched from.
# Required when vulnerability.policy.bundle.url is set.
#
# @category:     General
# @type:         enum
# @valid-values: [nginx, s3]
vulnerability.policy.bundle.source.type=NGINX

# For nginx server, if username and bearer token both are provided, basic auth will be used,
# else the auth header will be added based on the not null values
# Defines the password to be used for basic authentication against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.auth.password=

# Defines the username to be used for basic authentication against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.auth.username=

# Defines the token to be used as bearerAuth against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.bearer.token=

# S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional
#
# @category: General
# @type:     string
vulnerability.policy.s3.access.key=

# @category: General
# @type:     string
vulnerability.policy.s3.secret.key=

# @category: General
# @type:     string
vulnerability.policy.s3.bucket.name=

# @category: General
# @type:     string
vulnerability.policy.s3.bundle.name=

# @category: General
# @type:     string
vulnerability.policy.s3.region=

# Whether to execute initialization tasks on startup.
# Initialization tasks include:
# <ul>
#   <li>Execution of database migrations</li>
#   <li>Populating the database with default objects (permissions, users, licenses, etc.)</li>
# </ul>
#
# @category: General
# @type:     boolean
init.tasks.enabled=true

# Whether to only execute initialization tasks and exit.
#
# @category: General
# @type:     boolean
init.and.exit=false

# Whether dev services shall be enabled.
# <br/><br/>
# When enabled, Dependency-Track will automatically launch containers for:
# <ul>
#   <li>Frontend</li>
#   <li>Kafka</li>
#   <li>PostgreSQL</li>
# </ul>
# at startup, and configures itself to use them. They are disposed when
# Dependency-Track stops. The containers are exposed on randomized ports,
# which will be logged during startup.
# <br/><br/>
# Trying to enable dev services in a production build will prevent
# the application from starting.
# <br/><br/>
# Note that the containers launched by the API server can not currently
# be discovered and re-used by other Hyades services. This is a future
# enhancement tracked in <https://github.com/DependencyTrack/hyades/issues/1188>.
#
# @category: Development
# @type:     boolean
dev.services.enabled=false

# The image to use for the frontend dev services container.
#
# @category: Development
# @type:     string
dev.services.image.frontend=ghcr.io/dependencytrack/hyades-frontend:snapshot

# The image to use for the Kafka dev services container.
#
# @category: Development
# @type:     string
dev.services.image.kafka=apache/kafka-native:3.9.0

# The image to use for the PostgreSQL dev services container.
#
# @category: Development
# @type:     string
dev.services.image.postgres=postgres:13-alpine

# The port on which the frontend dev services container shall be exposed on the host.
#
# @category: Development
# @type:     integer
dev.services.port.frontend=8081

# The port on which the Kafka dev services container shall be exposed on the host.
#
# @category: Development
# @type:     integer
dev.services.port.kafka=9092

# The port on which the PostgreSQL dev services container shall be exposed on the host.
#
# @category: Development
# @type:     integer
dev.services.port.postgres=5432

# Cron expression of the component metadata maintenance task.
# <br/><br/>
# The task deletes orphaned records from the `INTEGRITY_META_COMPONENT` and
# `REPOSITORY_META_COMPONENT` tables.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.component.metadata.maintenance.cron=0 */12 * * *

# Maximum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.component.metadata.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.component.metadata.maintenance.lock.min.duration=PT1M

# Cron expression of the metrics maintenance task.
# <br/><br/>
# The task creates new partitions for the day for the following tables
# And deletes records older than the configured metrics retention duration from the following tables:
# <ul>
#   <li><code>DEPENDENCYMETRICS</code></li>
#   <li><code>PROJECTMETRICS</code></li>
#   <li><code>PORTFOLIOMETRICS</code></li>
# </ul>
#
# @category: Task Scheduling
# @type:     cron
# @required
task.metrics.maintenance.cron=1 * * * *

# Maximum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.metrics.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.metrics.maintenance.lock.min.duration=PT1M

# Cron expression of the tag maintenance task.
# <br/><br/>
# The task deletes orphaned tags that are not used anymore.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.tag.maintenance.cron=0 */12 * * *

# Maximum duration in ISO 8601 format for which the tag maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.tag.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the tag maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.tag.maintenance.lock.min.duration=PT1M

# Cron expression of the vulnerability database maintenance task.
# <br/><br/>
# The task deletes orphaned records from the `VULNERABLESOFTWARE` table.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.database.maintenance.cron=0 0 * * *

# Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.database.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.database.maintenance.lock.min.duration=PT1M

# Cron expression of the vulnerability scan maintenance task.
# <br/><br/>
# The task deletes records older than the configured retention duration from the `VULNERABILITYSCAN` table.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.scan.maintenance.cron=0 * * * *

# Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.scan.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.scan.maintenance.lock.min.duration=PT1M

# Cron expression of the workflow maintenance task.
# <br/><br/>
# The task:
# <ul>
#   <li>Transitions workflow steps from <code>PENDING</code> to <code>TIMED_OUT</code> state</li>
#   <li>Transitions workflow steps from <code>TIMED_OUT</code> to <code>FAILED</code> state</li>
#   <li>Transitions children of <code>FAILED</code> steps to <code>CANCELLED</code> state</li>
#   <li>Deletes finished workflows according to the configured retention duration</li>
# </ul>
#
# @category: Task Scheduling
# @type:     cron
# @required
task.workflow.maintenance.cron=*/15 * * * *

# Maximum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.workflow.maintenance.lock.max.duration=PT5M

# Minimum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.workflow.maintenance.lock.min.duration=PT1M

# Cron expression of the project maintenance task.
# <br/><br/>
# The task deletes inactive projects based on retention policy.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.project.maintenance.cron=0 */4 * * *

# Maximum duration in ISO 8601 format for which the project maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.project.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the project maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.project.maintenance.lock.min.duration=PT1M

# Defines the file storage extension to use.
# When not set, an enabled extension will be chosen based on its priority.
# It is recommended to explicitly configure an extension for predictable behavior.
#
# @category:     Storage
# @type:         enum
# @valid-values: [local, memory, s3]
# file.storage.default.extension=

# Whether the local file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
file.storage.extension.local.enabled=true

# Defines the local directory where files shall be stored.
# Has no effect unless file.storage.extension.local.enabled is `true`.
#
# @category: Storage
# @default:  ${alpine.data.directory}/storage
# @type:     string
# file.storage.extension.local.directory=

# Defines the size threshold for files after which they will be compressed.
# Compression is performed using the zstd algorithm.
# Has no effect unless file.storage.extension.local.enabled is `true`.
#
# @category: Storage
# @default:  4096
# @type:     integer
# file.storage.extension.local.compression.threshold.bytes=

# Defines the zstd compression level to use.
# Has no effect unless file.storage.extension.local.enabled is `true`.
#
# @category:     Storage
# @default:      5
# @type:         integer
# @valid-values: [-7..22]
# file.storage.extension.local.compression.level=

# Whether the in-memory file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
file.storage.extension.memory.enabled=false

# Whether the s3 file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
file.storage.extension.s3.enabled=false

# Defines the S3 endpoint URL.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# file.storage.extension.s3.endpoint=

# Defines the name of the S3 bucket.
# The existence of the bucket will be verified during startup,
# even when S3 is not configured as default extension.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# file.storage.extension.s3.bucket=

# Defines the S3 access key / username.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# file.storage.extension.s3.access.key=

# Defines the S3 secret key / password.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# file.storage.extension.s3.secret.key=

# Defines the region of the S3 bucket.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# file.storage.extension.s3.region=

# Defines the size threshold for files after which they will be compressed.
# Compression is performed using the zstd algorithm.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category: Storage
# @default:  4096
# @type:     integer
# file.storage.extension.s3.compression.threshold.bytes=

# Defines the zstd compression level to use.
# Has no effect unless file.storage.extension.s3.enabled is `true`.
#
# @category:     Storage
# @default:      5
# @type:         integer
# @valid-values: [-7..22]
# file.storage.extension.s3.compression.level=