# Defines the number of worker threads that the event subsystem will consume.
# Events occur asynchronously and are processed by the Event subsystem. This
# value should be large enough to handle most production situations without
# introducing much delay, yet small enough not to pose additional load on an
# already resource-constrained server.
# A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This
# can further be tweaked using the alpine.worker.thread.multiplier property.
#
# @category: Task Execution
# @type:     integer
# @required
alpine.worker.threads=0

# Defines a multiplier that is used to calculate the number of threads used
# by the event subsystem. This property is only used when alpine.worker.threads
# is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)
# 16 worker threads.
#
# @category: Task Execution
# @type:     integer
# @required
alpine.worker.thread.multiplier=4

# Defines the path to the data directory. This directory will hold logs,
# keys, and any database or index files along with application-specific
# files or directories.
#
# @category: General
# @type:     string
# @required
alpine.data.directory=${user.home}/.dependency-track

# Defines the path to the secret key to be used for data encryption and decryption.
# The key will be generated upon first startup if it does not exist.
#
# @category: General
# @type:     string
alpine.secret.key.path=${alpine.data.directory}/keys/secret.key

# Defines the prefix to be used for API keys. A maximum prefix length of 251
# characters is supported. The prefix may also be left empty.
#
# @category: General
# @type:     string
alpine.api.key.prefix=odt_

# Defines the number of seconds for which JWTs issued by Dependency-Track will be valid for.
#
# @category: General
# @type:     integer
alpine.auth.jwt.ttl.seconds=604800

# Defines the JDBC URL to use for the default data source.
#
# @category: Database
# @example:  jdbc:postgresql://localhost:5432/dtrack?reWriteBatchedInserts=true
# @type:     string
# @required
dt.datasource.url=${alpine.database.url}

# Defines the username to use for the default data source.
#
# @category: Database
# @type:     string
dt.datasource.username=${alpine.database.username}

# Defines the password to use for the default data source.
#
# @category: Database
# @type:     string
dt.datasource.password=${alpine.database.password}

# Defines the location of the file to load the password for the default data source from.
# If set, takes precedence over dt.datasource.password.
#
# @category: Database
# @type:     string
dt.datasource.password-file=${alpine.database.password.file}

# Defines whether connection pooling is enabled for the default data source.
#
# @category: Database
# @type:     boolean
# @required
dt.datasource.pool.enabled=${alpine.database.pool.enabled}

# Defines the maximum size of the connection pool for the default data source.
#
# Required when dt.datasource.pool.enabled is `true`.
#
# @category: Database
# @type:     integer
dt.datasource.pool.max-size=${alpine.database.pool.max.size}

# Defines the minimum number of idle connections in the pool for the default data source.
#
# Required when dt.datasource.pool.enabled is `true`.
#
# @category: Database
# @type:     integer
dt.datasource.pool.min-idle=${alpine.database.pool.min.idle}

# Defines the maximum time in milliseconds that a connection is allowed to sit idle in the pool.
#
# @category:   Database
# @type:       integer
dt.datasource.pool.idle-timeout-ms=${alpine.database.pool.idle.timeout}

# Defines the maximum time in milliseconds for which connections should be kept in the pool for the default data source.
#
# Required when dt.datasource.pool.enabled is `true`.
#
# @category: Database
# @type:     integer
dt.datasource.pool.max-lifetime-ms=${alpine.database.pool.max.lifetime}

# Specifies the JDBC URL to use when connecting to the database.
# For best performance, set the `reWriteBatchedInserts` query parameter to `true`.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.url instead.
# @example:    jdbc:postgresql://localhost:5432/dtrack?reWriteBatchedInserts=true
# @type:       string
alpine.database.url=

# Specifies the JDBC driver class to use.
#
# @category: Database
# @type:     string
# @hidden
alpine.database.driver=org.postgresql.Driver

# Specifies the username to use when authenticating to the database.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.username instead.
# @type:       string
alpine.database.username=dtrack

# Specifies the password to use when authenticating to the database.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.password instead.
# @type:       string
alpine.database.password=dtrack

# Specifies the file to load the database password from.
# If set, takes precedence over alpine.database.password.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.password-file instead.
# @example:    /var/run/secrets/database-password
# @type:       string
# alpine.database.password.file=

# Specifies if the database connection pool is enabled.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.pool.enabled instead.
# @type:       boolean
alpine.database.pool.enabled=true

# This property controls the maximum size that the pool is allowed to reach,
# including both idle and in-use connections.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.pool.max-size instead.
# @type:       integer
alpine.database.pool.max.size=20

# This property controls the minimum number of idle connections in the pool.
# This value should be equal to or less than alpine.database.pool.max.size.
# Warning: If the value is less than alpine.database.pool.max.size,
# alpine.database.pool.idle.timeout will have no effect.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.pool.min-idle instead.
# @type:       integer
alpine.database.pool.min.idle=10

# This property controls the maximum amount of time that a connection is
# allowed to sit idle in the pool.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.pool.idle-timeout-ms instead.
# @type:       integer
alpine.database.pool.idle.timeout=300000

# This property controls the maximum lifetime of a connection in the pool.
# An in-use connection will never be retired, only when it is closed will
# it then be removed.
#
# @category:   Database
# @deprecated: Since 5.7.0. Use dt.datasource.pool.max-lifetime-ms instead.
# @type:       integer
alpine.database.pool.max.lifetime=600000

# Controls the maximum number of ExecutionContext objects that are pooled by DataNucleus.
# The default defined by DataNucleus is 20. However, since Dependency-Track occasionally
# tweaks ExecutionContexts (e.g. to disable auto-flushing for insert-heavy tasks),
# they are not safe for reuse. We thus disable EC pooling completely.
#
# @category: Database
# @type:     integer
# @hidden
alpine.datanucleus.executioncontext.maxidle=0

# Controls the deletion policy utilized by DataNucleus.
# The default `JDO2` does not consider FK constraints for cascading deletes, only `DataNucleus` does.
# Refer to https://www.datanucleus.org/products/accessplatform/jdo/persistence.html#_deleting_an_object for details.
#
# @category:     Database
# @type:         string
# @valid-values: [JDO2, DataNucleus]
# @hidden
alpine.datanucleus.deletionpolicy=DataNucleus

# Defines the name of the data source to be used by init tasks.
#
# @category: Database
# @type:     string
# @required
init.tasks.datasource.name=default

# Defines whether the data source used by init tasks should be closed
# after all tasks completed. This is useful when a non-default data source
# was configured, that is not used anywhere else.
#
# @category: Database
# @type:     boolean
# @required
init.tasks.datasource.close-after-use=false

# Defines the secret management type to use.
#
# @category:     Secrets
# @type:         enum
# @valid-values: [database, env]
# @required
dt.secret-management.provider=database

# Defines the name of the data source to be used by the database secret manager.
# <br/><br/>
# Required when dt.secret-management.provider is `database`.
#
# @category: Secrets
# @type:     string
dt.secret-management.database.datasource.name=default

# Defines the path to the key encryption keyset to use for the database secret manager.
# <br/><br/>
# Required when dt.secret-management.provider is `database`.
#
# @category: Secrets
# @type:     string
dt.secret-management.database.kek-keyset.path=${alpine.data.directory}/keys/secret-management-kek.json

# Defines whether a key encryption keyset should be created if it doesn't already exist.
#
# @category: Secrets
# @type:     boolean
dt.secret-management.database.kek-keyset.create-if-missing=true

# Defines whether secret caching should be enabled.
#
# @category: Secrets
# @type:     boolean
# @required
dt.secret-management.cache.enabled=false

# Defines the duration in milliseconds for which secrets should be cached.
#
# @category: Secrets
# @type:     integer
dt.secret-management.cache.expire-after-write-ms=60000

# Defines the maximum number of secrets to keep in the cache.
#
# @category: Secrets
# @type:     integer
dt.secret-management.cache.max-size=100

# Specifies the number of bcrypt rounds to use when hashing a user's password.
# The higher the number the more secure the password, at the expense of
# hardware resources and additional time to generate the hash.
#
# @category: General
# @type:     integer
# @required
alpine.bcrypt.rounds=14

# Defines if LDAP will be used for user authentication. If enabled,
# `alpine.ldap.*` properties should be set accordingly.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.enabled=false

# Specifies the LDAP server URL.
# <br/><br/>
# Examples (Microsoft Active Directory):
# <ul>
#   <li><code>ldap://ldap.example.com:3268</code></li>
#   <li><code>ldaps://ldap.example.com:3269</code></li>
# </ul>
# Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul>
#   <li><code>ldap://ldap.example.com:389</code></li>
#   <li><code>ldaps://ldap.example.com:636</code></li>
# </ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.server.url=

# Specifies the base DN that all queries should search from
#
# @category: LDAP
# @example:  dc=example,dc=com
# @type:     string
alpine.ldap.basedn=

# Specifies the LDAP security authentication level to use. Its value is one of
# the following strings: "none", "simple", "strong". If this property is empty
# or unspecified, the behaviour is determined by the service provider.
#
# @category:     LDAP
# @type:         enum
# @valid-values: [none, simple, strong]
alpine.ldap.security.auth=simple

# If anonymous access is not permitted, specify a username with limited access
# to the directory, just enough to perform searches. This should be the fully
# qualified DN of the user.
#
# @category: LDAP
# @type:     string
alpine.ldap.bind.username=

# If anonymous access is not permitted, specify a password for the username
# used to bind.
#
# @category: LDAP
# @type:     string
alpine.ldap.bind.password=

# Specifies if the username entered during login needs to be formatted prior
# to asserting credentials against the directory. For Active Directory, the
# userPrincipal attribute typically ends with the domain, whereas the
# samAccountName attribute and other directory server implementations do not.
# The %s variable will be substituted with the username asserted during login.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>%s@example.com</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>%s</code></li></ul>
#
# @category: LDAP
# @example:  %s@example.com
# @type:     string
alpine.ldap.auth.username.format=

# Specifies the Attribute that identifies a users ID.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>userPrincipalName</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>uid</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.attribute.name=userPrincipalName

# Specifies the LDAP attribute used to store a users email address
#
# @category: LDAP
# @type:     string
alpine.ldap.attribute.mail=mail

# Specifies the LDAP search filter used to retrieve all groups from the directory.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group))

# Specifies the LDAP search filter to use to query a user and retrieve a list
# of groups the user is a member of. The `{USER_DN}` variable will be substituted
# with the actual value of the users DN at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>
# Example (Microsoft Active Directory - with nested group support):
# <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN})

# Specifies the LDAP search filter used to search for groups by their name.
# The `{SEARCH_TERM}` variable will be substituted at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=groupOfUniqueNames)(cn=*{SEARCH_TERM}*))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))

# Specifies the LDAP search filter used to search for users by their name.
# The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.
# <br/><br/>
# Example (Microsoft Active Directory):
# <ul><li><code>(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code></li></ul>
# Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):
# <ul><li><code>(&(objectClass=inetOrgPerson)(cn=*{SEARCH_TERM}*))</code></li></ul>
#
# @category: LDAP
# @type:     string
alpine.ldap.users.search.filter=(&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))

# Specifies if mapped LDAP accounts are automatically created upon successful
# authentication. When a user logs in with valid credentials but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which ldap users can access the
# system and which users cannot. When this value is set to true, a local ldap
# user will be created and mapped to the ldap account automatically. This
# automatic provisioning only affects authentication, not authorization.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.user.provisioning=false

# This option will ensure that team memberships for LDAP users are dynamic and
# synchronized with membership of LDAP groups. When a team is mapped to an LDAP
# group, all local LDAP users will automatically be assigned to the team if
# they are a member of the group the team is mapped to. If the user is later
# removed from the LDAP group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via an
# external directory.
#
# @category: LDAP
# @type:     boolean
alpine.ldap.team.synchronization=false

# HTTP proxy address. If set, then alpine.http.proxy.port must be set too.
#
# @category: HTTP
# @example:  proxy.example.com
# @type:     string
# alpine.http.proxy.address=

# @category: HTTP
# @example:  8888
# @type:     integer
# alpine.http.proxy.port=

# @category: HTTP
# @type:     string
# alpine.http.proxy.username=

# @category: HTTP
# @type:     string
# alpine.http.proxy.password=

# Specifies the file to load the HTTP proxy password from.
# If set, takes precedence over alpine.http.proxy.password.
#
# @category: HTTP
# @example:  /var/run/secrets/http-proxy-password
# @type:     string
# alpine.http.proxy.password.file=

# @category: HTTP
# @example:  localhost,127.0.0.1
# @type:     string
# alpine.no.proxy=

# Defines the connection timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.connection=30

# Defines the socket / read timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.socket=30

# Defines the request timeout in seconds for outbound HTTP connections.
#
# @category: HTTP
# @type:     integer
# alpine.http.timeout.pool=60

# Defines whether [Cross Origin Resource Sharing](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
# (CORS) headers shall be included in REST API responses.
#
# @category: CORS
# @type:     boolean
# alpine.cors.enabled=true

# Controls the content of the `Access-Control-Allow-Origin` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.origin=*

# Controls the content of the `Access-Control-Allow-Methods` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.methods=GET,POST,PUT,PATCH,DELETE,OPTIONS

# Controls the content of the `Access-Control-Allow-Headers` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.allow.headers=Origin,Content-Type,Authorization,X-Requested-With,Content-Length,Accept,Origin,X-Api-Key,X-Total-Count,*

# Controls the content of the `Access-Control-Expose-Headers` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     string
# alpine.cors.expose.headers=Origin,Content-Type,Authorization,X-Requested-With,Content-Length,Accept,Origin,X-Api-Key,X-Total-Count

# Controls the content of the `Access-Control-Allow-Credentials` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     boolean
# alpine.cors.allow.credentials=true

# Controls the content of the `Access-Control-Max-Age` response header.
# <br/>
# Has no effect when alpine.cors.enabled is `false`.
#
# @category: CORS
# @type:     integer
# alpine.cors.max.age=3600

# Defines whether Prometheus metrics will be exposed.
# If enabled, metrics will be available via the /metrics endpoint.
#
# @category:   Observability
# @deprecated: Since 5.7.0. Use dt.metrics.enabled instead.
# @type:       boolean
alpine.metrics.enabled=

# Defines the username required to access metrics.
# Has no effect when alpine.metrics.auth.password is not set.
#
# @category:   Observability
# @deprecated: Since 5.7.0. Use dt.metrics.auth.username instead.
# @type:       string
alpine.metrics.auth.username=

# Defines the password required to access metrics.
# Has no effect when alpine.metrics.auth.username is not set.
#
# @category:   Observability
# @deprecated: Since 5.7.0. Use dt.metrics.auth.password instead.
# @type:       string
alpine.metrics.auth.password=

# Defines whether Prometheus metrics will be exposed.
# If enabled, metrics will be available via the /metrics endpoint.
#
# @category: Observability
# @type:     boolean
dt.metrics.enabled=false

# Defines the username required to access metrics.
# Has no effect when dt.metrics.auth.password is not set.
#
# @category: Observability
# @type:     string
dt.metrics.auth.username=

# Defines the password required to access metrics.
# Has no effect when dt.metrics.auth.username is not set.
#
# @category: Observability
# @type:     string
dt.metrics.auth.password=

# Defines if OpenID Connect will be used for user authentication.
# If enabled, `alpine.oidc.*` properties should be set accordingly.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.enabled=false

# Defines the client ID to be used for OpenID Connect.
# The client ID should be the same as the one configured for the frontend,
# and will only be used to validate ID tokens.
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.client.id=

# Defines the issuer URL to be used for OpenID Connect.
# This issuer MUST support provider configuration via the `/.well-known/openid-configuration` endpoint.
# See also:
# <ul>
#   <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li>
#   <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li>
# </ul>
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.issuer=

# Defines the name of the claim that contains the username in the provider's userinfo endpoint.
# Common claims are `name`, `username`, `preferred_username` or `nickname`.
# See also:
# <ul>
#   <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li>
# </ul>
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.username.claim=name

# Specifies if mapped OpenID Connect accounts are automatically created upon successful
# authentication. When a user logs in with a valid access token but an account has
# not been previously provisioned, an authentication failure will be returned.
# This allows admins to control specifically which OpenID Connect users can access the
# system and which users cannot. When this value is set to true, a local OpenID Connect
# user will be created and mapped to the OpenID Connect account automatically. This
# automatic provisioning only affects authentication, not authorization.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.user.provisioning=false

# This option will ensure that team memberships for OpenID Connect users are dynamic and
# synchronized with membership of OpenID Connect groups or assigned roles. When a team is
# mapped to an OpenID Connect group, all local OpenID Connect users will automatically be
# assigned to the team if they are a member of the group the team is mapped to. If the user
# is later removed from the OpenID Connect group, they will also be removed from the team. This
# option provides the ability to dynamically control user permissions via the identity provider.
# Note that team synchronization is only performed during user provisioning and after successful
# authentication.
#
# @category: OpenID Connect
# @type:     boolean
alpine.oidc.team.synchronization=false

# Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.
# The claim must be an array of strings, or a comma-delimited string. Most public identity providers do not support group or role management.
# When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint
# will most likely need to be configured.
#
# @category: OpenID Connect
# @type:     string
alpine.oidc.teams.claim=groups

# Defines whether the notification outbox relay should be enabled.
# When disabled, notifications will still be emitted to the outbox
# table, but not be delivered. Should generally stay enabled, unless:
# <ul>
#   <li>The relay has a critical issue that impacts the rest of the system</li>
#   <li>You run a multi-node cluster and want more granular control over which nodes run the relay</li>
# </ul>
#
# @category: Notification
# @type:     boolean
# @required
notification.outbox-relay.enabled=true

# Defines the interval in milliseconds in which the notification outbox relay will poll
# for records in the notification outbox table. Increasing this value will cause higher
# notification latencies, but incurs a lesser load on the database.
#
# @category: Notification
# @type:     integer
# @required
notification.outbox-relay.poll-interval-ms=1000

# Defines the number of notifications that the outbox relay will process in a batch.
#
# @category: Notification
# @type:     integer
# @required
notification.outbox-relay.batch-size=100

# Defines the size in bytes at which notifications are considered "large".
# <br/><br/>
# Large notifications will be offloaded to file storage before
# being sent to the dex engine for publishing.
#
# @category: Notification
# @type:     integer
# @required
notification.outbox-relay.large-notification-threshold-bytes=65536

# @category: Kafka
# @example:  localhost:9092
# @type:     string
# @required
kafka.bootstrap.servers=

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
kafka.auto.offset.reset=earliest

# @category: Kafka
# @type:     boolean
kafka.tls.enabled=false

# @category: Kafka
# @type:     boolean
kafka.mtls.enabled=false

# @category:     Kafka
# @type:         enum
# @valid-values: [PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]
kafka.security.protocol=

# @category: Kafka
# @type:     string
kafka.truststore.path=

# @category: Kafka
# @type:     string
kafka.truststore.password=

# @category: Kafka
# @type:     string
kafka.keystore.path=

# @category: Kafka
# @type:     string
kafka.keystore.password=

# @category: Kafka
# @type:     string
dt.kafka.topic.prefix=

# Defines the order in which records are being processed.
# Valid options are:
#  * partition
#  * key
#  * unordered
# kafka.processor.<name>.processing.order=partition

# Defines the maximum size of record batches being processed.
# Batch sizes are further limited by the configured processing order:
#  * partition: Number of partitions assigned to this processor
#  * key:       Number of distinct record keys in current consumer poll
#  * unordered: Potentially unlimited
# Will be ignored when the processor is not a batch processor.
# kafka.processor.<name>.max.batch.size=10

# Defines the maximum concurrency with which records are being processed.
# For batch processors, a smaller number can improve efficiency and throughput.
# A value of -1 indicates that the maximum concurrency should be equal to
# the number of partitions in the topic being consumed from.
# kafka.processor.<name>.max.concurrency=1

# Allows for customization of the processor's retry behavior.
# kafka.processor.<name>.retry.initial.delay.ms=1000
# kafka.processor.<name>.retry.multiplier=1
# kafka.processor.<name>.retry.randomization.factor=0.3
# kafka.processor.<name>.retry.max.delay.ms=60000

# Defines the timeout to wait for the processor to finish any pending work
# prior to being shut down.
# kafka.processor.<name>.shutdown.timeout.ms=10000

# Allows for customization of the underlying Kafka consumer.
# Refer to https://kafka.apache.org/documentation/#consumerconfigs for available options.
# kafka.processor.<name>.consumer.<consumer.config.name>=

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.mirror.processing.order=partition

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.initial.delay.ms=3000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.mirror.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.mirror.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.mirror.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.mirror.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.repo.meta.analysis.result.processing.order=key

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.repo.meta.analysis.result.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.repo.meta.analysis.result.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.repo.meta.analysis.result.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.max.concurrency=-1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.scan.result.processing.order=key

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.initial.delay.ms=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.scan.result.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.scan.result.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.scan.result.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.max.batch.size=1000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.max.concurrency=1

# @category:     Kafka
# @type:         enum
# @valid-values: [key, partition, unordered]
# @required
kafka.processor.vuln.scan.result.processed.processing.order=unordered

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms=3000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.multiplier=2

# @category: Kafka
# @type:      double
# @required
kafka.processor.vuln.scan.result.processed.retry.randomization.factor=0.3

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.retry.max.delay.ms=180000

# @category: Kafka
# @type:     string
# @required
kafka.processor.vuln.scan.result.processed.consumer.group.id=dtrack-apiserver-processor

# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
# @required
kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset=earliest

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.consumer.max.poll.records=10000

# @category: Kafka
# @type:     integer
# @required
kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes=524288

# Maximum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.portfolio.metrics.update.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.portfolio.metrics.update.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.metrics.update.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.metrics.update.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the internal component identification task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.internal.component.identification.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the internal component identification task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.internal.component.identification.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.ldap.sync.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.ldap.sync.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.repository.meta.analysis.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.repository.meta.analysis.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.analysis.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.analysis.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.integrity.meta.initializer.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.integrity.meta.initializer.lock.min.duration=PT90S

# Maximum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.policy.fetch.lock.max.duration=PT5M

# Minimum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.policy.fetch.lock.min.duration=PT5S

# Cron expression of the portfolio metrics update task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.portfolio.metrics.update.cron=10 * * * *

# Cron expression of the vulnerability metrics update task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.metrics.update.cron=40 * * * *

# Maximum duration in ISO 8601 format for which the CSAF mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.csaf.document.import.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the CSAF mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.csaf.document.import.lock.min.duration=PT1M

# Cron expression of the CSAF mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.csaf.document.import.cron=0 5 * * *

# Cron expression of the vulnerability GitHub Advisories mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.git.hub.advisory.mirror.cron=0 2 * * *

# Maximum duration in ISO 8601 format for which the GitHub mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.git.hub.advisory.mirror.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the GitHub mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.git.hub.advisory.mirror.lock.min.duration=PT1M

# Cron expression of the OSV mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.osv.mirror.cron=0 3 * * *

# Maximum duration in ISO 8601 format for which the OSV mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.osv.mirror.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the OSV mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.osv.mirror.lock.min.duration=PT1M

# Cron expression of the NIST / NVD mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.nist.mirror.cron=0 4 * * *

# Maximum duration in ISO 8601 format for which the NIST mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.nist.mirror.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the NIST mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.nist.mirror.lock.min.duration=PT1M

# Cron expression of the EPSS mirroring task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.epss.mirror.cron=0 1 * * *

# Maximum duration in ISO 8601 format for which the EPSS mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.epss.mirror.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the EPSS mirror task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.epss.mirror.lock.min.duration=PT1M

# Cron expression of the internal component identification task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.internal.component.identification.cron=25 */6 * * *

# Cron expression of the LDAP synchronization task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.ldap.sync.cron=0 */6 * * *

# Cron expression of the portfolio repository metadata analysis task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.repository.meta.analysis.cron=0 1 * * *

# Cron expression of the portfolio vulnerability analysis task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.analysis.cron=0 6 * * *

# Cron expression of the vulnerability policy bundle fetch task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.policy.fetch.cron=*/5 * * * *

# Cron expression of the Fortify SSC upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.fortify.ssc.upload.cron=0 2 * * *

# Cron expression of the DefectDojo upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.defect.dojo.upload.cron=0 2 * * *

# Cron expression of the Kenna Security upload task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.kenna.security.upload.cron=0 2 * * *

# Cron expression of the integrity metadata initializer task.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.integrity.meta.initializer.cron=0 */12 * * *

# Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload
# is completed. The intention being that it is then "safe" to query the API for any identified vulnerabilities.
# This is specifically for cases where polling the /api/v1/bom/token/<TOKEN> endpoint is not feasible.
# THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.
#
# @category: General
# @type:     boolean
tmp.delay.bom.processed.notification=false

# Specifies whether the Integrity Initializer shall be enabled.
#
# @category: General
# @type:     boolean
integrity.initializer.enabled=false

# @category: General
# @type:     boolean
integrity.check.enabled=false

# Defines whether vulnerability policy analysis is enabled.
#
# @category: General
# @type:     boolean
vulnerability.policy.analysis.enabled=false

# Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port
# For nginx, the whole url with bundle name needs to be given
#
# @category: General
# @example:  http://example.com:80/bundles/bundle.zip
# @type:     string
vulnerability.policy.bundle.url=

# Defines the type of source from which policy bundles are being fetched from.
# Required when vulnerability.policy.bundle.url is set.
#
# @category:     General
# @type:         enum
# @valid-values: [nginx, s3]
vulnerability.policy.bundle.source.type=NGINX

# For nginx server, if username and bearer token both are provided, basic auth will be used,
# else the auth header will be added based on the not null values
# Defines the password to be used for basic authentication against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.auth.password=

# Defines the username to be used for basic authentication against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.auth.username=

# Defines the token to be used as bearerAuth against the service hosting the policy bundle.
#
# @category: General
# @type:     string
vulnerability.policy.bundle.bearer.token=

# S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional
#
# @category: General
# @type:     string
vulnerability.policy.s3.access.key=

# @category: General
# @type:     string
vulnerability.policy.s3.secret.key=

# @category: General
# @type:     string
vulnerability.policy.s3.bucket.name=

# @category: General
# @type:     string
vulnerability.policy.s3.bundle.name=

# @category: General
# @type:     string
vulnerability.policy.s3.region=

# Whether to execute initialization tasks on startup.
#
# @category: General
# @type:     boolean
init.tasks.enabled=true

# Whether to enable the database migration init task.
#
# Has no effect unless init.tasks.enabled is `true`.
#
# @category: General
# @type:     boolean
init.task.database.migration.enabled=true

# Whether to enable the database partition maintenance init task.
#
# Has no effect unless init.tasks.enabled is `true`.
#
# @category: General
# @type:     boolean
init.task.database.partition.maintenance.enabled=true

# Whether to enable the database seeding init task.
#
# Seeding involves populating the database with default objects,
# such as permissions, users, licenses, etc.
#
# Has no effect unless init.tasks.enabled is `true`.
#
# @category: General
# @type:     boolean
init.task.database.seeding.enabled=true

# Whether to enable the key generation init task.
#
# Has no effect unless init.tasks.enabled is `true`.
#
# @category: General
# @type:     boolean
init.task.key.generation.enabled=true

# Whether to enable the durable execution engine database migration init task.
#
# Has no effect unless init.tasks.enabled is `true`.
#
# @category: General
# @type:     boolean
init.task.dex.engine.database.migration.enabled=true

# Whether to only execute initialization tasks and exit.
#
# @category: General
# @type:     boolean
init.and.exit=false

# Whether dev services shall be enabled.
# <br/><br/>
# When enabled, Dependency-Track will automatically launch containers for:
# <ul>
#   <li>Frontend</li>
#   <li>Kafka</li>
#   <li>PostgreSQL</li>
# </ul>
# at startup, and configures itself to use them. They are disposed when
# Dependency-Track stops. The containers are exposed on randomized ports,
# which will be logged during startup.
# <br/><br/>
# Trying to enable dev services in a production build will prevent
# the application from starting.
# <br/><br/>
# Note that the containers launched by the API server can not currently
# be discovered and re-used by other Hyades services. This is a future
# enhancement tracked in <https://github.com/DependencyTrack/hyades/issues/1188>.
#
# @category: Development
# @type:     boolean
dev.services.enabled=false

# The image to use for the frontend dev services container.
#
# @category: Development
# @type:     string
dev.services.image.frontend=ghcr.io/dependencytrack/hyades-frontend:snapshot

# The image to use for the Kafka dev services container.
#
# @category: Development
# @type:     string
dev.services.image.kafka=apache/kafka-native:3.9.1

# The image to use for the PostgreSQL dev services container.
#
# @category: Development
# @type:     string
dev.services.image.postgres=postgres:14-alpine

# The port on which the frontend dev services container shall be exposed on the host.
#
# @category: Development
# @type:     integer
dev.services.port.frontend=8081

# The port on which the Kafka dev services container shall be exposed on the host.
#
# @category: Development
# @type:     integer
dev.services.port.kafka=9092

# Cron expression of the component metadata maintenance task.
# <br/><br/>
# The task deletes orphaned records from the `INTEGRITY_META_COMPONENT` and
# `REPOSITORY_META_COMPONENT` tables.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.component.metadata.maintenance.cron=0 */12 * * *

# Maximum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.component.metadata.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.component.metadata.maintenance.lock.min.duration=PT1M

# Cron expression of the metrics maintenance task.
# <br/><br/>
# The task creates new partitions for the day for the following tables
# And deletes records older than the configured metrics retention duration from the following tables:
# <ul>
#   <li><code>DEPENDENCYMETRICS</code></li>
#   <li><code>PROJECTMETRICS</code></li>
# </ul>
#
# @category: Task Scheduling
# @type:     cron
# @required
task.metrics.maintenance.cron=1 * * * *

# Maximum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.metrics.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.metrics.maintenance.lock.min.duration=PT1M

# Cron expression of the tag maintenance task.
# <br/><br/>
# The task deletes orphaned tags that are not used anymore.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.tag.maintenance.cron=0 */12 * * *

# Maximum duration in ISO 8601 format for which the tag maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.tag.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the tag maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.tag.maintenance.lock.min.duration=PT1M

# Cron expression of the vulnerability database maintenance task.
# <br/><br/>
# The task deletes orphaned records from the `VULNERABLESOFTWARE` table.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.database.maintenance.cron=0 0 * * *

# Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.database.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.database.maintenance.lock.min.duration=PT1M

# Cron expression of the vulnerability scan maintenance task.
# <br/><br/>
# The task deletes records older than the configured retention duration from the `VULNERABILITYSCAN` table.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.vulnerability.scan.maintenance.cron=0 * * * *

# Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.scan.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.vulnerability.scan.maintenance.lock.min.duration=PT1M

# Cron expression of the workflow maintenance task.
# <br/><br/>
# The task:
# <ul>
#   <li>Transitions workflow steps from <code>PENDING</code> to <code>TIMED_OUT</code> state</li>
#   <li>Transitions workflow steps from <code>TIMED_OUT</code> to <code>FAILED</code> state</li>
#   <li>Transitions children of <code>FAILED</code> steps to <code>CANCELLED</code> state</li>
#   <li>Deletes finished workflows according to the configured retention duration</li>
# </ul>
#
# @category: Task Scheduling
# @type:     cron
# @required
task.workflow.maintenance.cron=*/15 * * * *

# Maximum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.workflow.maintenance.lock.max.duration=PT5M

# Minimum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.workflow.maintenance.lock.min.duration=PT1M

# Cron expression of the project maintenance task.
# <br/><br/>
# The task deletes inactive projects based on retention policy.
#
# @category: Task Scheduling
# @type:     cron
# @required
task.project.maintenance.cron=0 */4 * * *

# Maximum duration in ISO 8601 format for which the project maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover the task's execution duration.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.project.maintenance.lock.max.duration=PT15M

# Minimum duration in ISO 8601 format for which the project maintenance task will hold a lock.
# <br/><br/>
# The duration should be long enough to cover eventual clock skew across API server instances.
#
# @category: Task Scheduling
# @type:     duration
# @required
task.project.maintenance.lock.min.duration=PT1M

# Defines the file storage extension to use.
# When not set, an enabled extension will be chosen based on its priority.
# It is recommended to explicitly configure an extension for predictable behavior.
#
# @category:     Storage
# @type:         enum
# @valid-values: [local, memory, s3]
# dt.file-storage.default-extension=

# Whether the local file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
dt.file-storage.local.enabled=true

# Defines the local directory where files shall be stored.
# Has no effect unless dt.file-storage.local.enabled is `true`.
#
# @category: Storage
# @type:     string
dt.file-storage.local.directory=${alpine.data.directory}/storage

# Defines the zstd compression level to use.
# Has no effect unless dt.file-storage.local.enabled is `true`.
#
# @category:     Storage
# @default:      5
# @type:         integer
# @valid-values: [-7..22]
# dt.file-storage.local.compression.level=

# Whether the in-memory file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
dt.file-storage.memory.enabled=false

# Whether the s3 file storage extension shall be enabled.
#
# @category: Storage
# @type:     boolean
dt.file-storage.s3.enabled=false

# Defines the S3 endpoint URL.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# dt.file-storage.s3.endpoint=

# Defines the name of the S3 bucket.
# The existence of the bucket will be verified during startup,
# even when S3 is not configured as default extension.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# dt.file-storage.s3.bucket=

# Defines the S3 access key / username.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# dt.file-storage.s3.access.key=

# Defines the S3 secret key / password.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# dt.file-storage.s3.secret.key=

# Defines the region of the S3 bucket.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category: Storage
# @type:     string
# dt.file-storage.s3.region=

# Defines the zstd compression level to use.
# Has no effect unless dt.file-storage.s3.enabled is `true`.
#
# @category:     Storage
# @default:      5
# @type:         integer
# @valid-values: [-7..22]
# dt.file-storage.s3.compression.level=

# Defines whether the console notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.console.enabled=true

# Defines whether the email notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.email.enabled=true

# Defines whether the email notification publisher is allowed to connect to local hosts.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.email.allow-local-connections=false

# Defines whether the Jira notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.jira.enabled=true

# Defines whether the Kafka notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.kafka.enabled=true

# Defines whether the Kafka notification publisher is allowed to connect to local hosts.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.kafka.allow-local-connections=false

# Defines whether the Mattermost notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.mattermost.enabled=true

# Defines whether the Microsoft Teams notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.msteams.enabled=true

# Defines whether the Slack notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.slack.enabled=true

# Defines whether the WebEx notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.webex.enabled=true

# Defines whether the Webhook notification publisher is enabled.
#
# @category: Notification
# @type:     boolean
# dt.notification-publisher.webhook.enabled=true

# Defines the name of the data source to be used by the durable execution engine.
#
# For larger deployments, it is recommended to use a separate,
# non-default data source.
#
# @category: Database
# @type:     string
dt.dex-engine.datasource.name=default

# Defines the name of the data source to use for executing database
# migrations of the durable execution engine.
#
# @category: Database
# @type:     string
# dt.dex-engine.migration.datasource.name=

# Defines the duration in milliseconds for which leadership leases are acquired.
#
# @category: Durable Execution
dt.dex-engine.leader-election.lease-duration-ms=30000

# Defines the interval in milliseconds in which leadership lease acquisition or extension is attempted.
# <br/><br/>
# Must be smaller than dt.dex-engine.leader-election.lease-duration-ms to avoid
# frequent leadership changes.
#
# @category: Durable Execution
dt.dex-engine.leader-election.lease-check-interval-ms=15000

# Defines the interval in milliseconds in which the workflow task scheduler polls
# for tasks to enqueue for execution.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.workflow-task-scheduler.poll-interval-ms=100

# Defines the interval in milliseconds in which the activity task scheduler polls
# for tasks to enqueue for execution.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.activity-task-scheduler.poll-interval-ms=100

# Defines whether the default workflow worker should be enabled.
#
# @category: Durable Execution
# @type:     boolean
# dt.dex-engine.workflow-worker.default.enabled=true

# @hidden
dt.dex-engine.workflow-worker.default.queue-name=default

# Defines the maximum concurrency of the default workflow worker.
# <br/><br/>
# Note that workflow workers do not perform any I/O (although they
# may block while waiting for semaphores and buffer flushes),
# and are executed with virtual threads. This means that it's
# usually perfectly fine to have a high degree of concurrency,
# without risking excessive resource usage or I/O thrashing.
#
# @category: Durable Execution
# @type:     integer
# @required
dt.dex-engine.workflow-worker.default.max-concurrency=100

# Defines whether the default activity worker should be enabled.
#
# @category: Durable Execution
# @type:     boolean
# dt.dex-engine.activity-worker.default.enabled=true

# @hidden
dt.dex-engine.activity-worker.default.queue-name=default

# Defines the maximum concurrency of the default activity worker.
#
# @category: Durable Execution
# @type:     integer
# @required
dt.dex-engine.activity-worker.default.max-concurrency=25

# Defines whether the notification activity worker should be enabled.
#
# @category: Durable Execution
# @type:     boolean
# dt.dex-engine.activity-worker.notification.enabled=true

# @hidden
dt.dex-engine.activity-worker.notification.queue-name=notifications

# Defines the maximum concurrency of the notification activity worker.
#
# @category: Durable Execution
# @type:     integer
# @required
dt.dex-engine.activity-worker.notification.max-concurrency=5

# Defines the time in milliseconds between flushes of the task event buffer.
# <br/><br/>
# Increasing this interval may yield better throughput while reducing the
# database load, but also increases end-to-end latency of workflow and
# activity executions.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.task-event-buffer.flush-interval-ms=100

# Defines the maximum number of items that will be flushed at once.
# <br/><br/>
# Increasing this value may yield better throughput,
# at the expense of higher latency and potentially larger
# blast radius in case a task event causes failures during the flush.
# <br/><br/>
# Since flushes are atomic, a single event failing to be flushed impacts
# the entire batch.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.task-event-buffer.max-batch-size=100

# Defines the time in milliseconds between flushes of the external event buffer.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.external-event-buffer.flush-interval-ms=100

# Defines the maximum number of items of the external event buffer.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.external-event-buffer.max-batch-size=100

# Defines the time in milliseconds between flushes of the activity task heartbeat buffer.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.activity-task-heartbeat-buffer.flush-interval-ms=100

# Defines the maximum number of items of the activity task heartbeat buffer.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.activity-task-heartbeat-buffer.max-batch-size=100

# Defines the time in milliseconds for which workflow run event histories are cached.
# <br/><br/>
# Histories are only cached for non-terminal runs, to improve performance of replay.
# Cached histories are automatically evicted when the corresponding run terminates.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.run-history-cache.evict-after-access-ms=300000

# Defines the maximum number of workflow runs for which histories may be cached.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.run-history-cache.max-size=1000

# Defines the initial delay in milliseconds after which the maintenance worker will execute for the first time.
# <br/><br/>
# Note that only the leader node in the cluster will actually perform maintenance work.
# For nodes that are not leaders, maintenance is a no-op.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.maintenance.worker.initial-delay-ms=60000

# Defines the interval in milliseconds at which the maintenance worker will execute.
# <br/><br/>
# Note that only the leader node in the cluster will actually perform maintenance work.
# For nodes that are not leaders, maintenance is a no-op.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.maintenance.worker.interval-ms=1800000

# Defines the duration in ISO 8601 format after which completed workflow runs become
# eligible for deletion.
#
# @category: Durable Execution
# @type:     duration
dt.dex-engine.maintenance.run-retention-duration=P1D

# Defines the maximum number of completed workflow runs to delete during a single execution
# of the maintenance worker. Deletion of large volumes of runs in one pass can lead to I/O
# spikes and increased table bloat.
# <br/><br/>
# If retention is not able to keep up with the volumes of
# runs, consider increasing the interval of the maintenance worker first.
#
# @category: Durable Execution
# @type:     integer
dt.dex-engine.maintenance.run-deletion-batch-size=1000
